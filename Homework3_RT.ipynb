{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6145,"status":"ok","timestamp":1648665200505,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"},"user_tz":240},"id":"PcV-ObZw8ghA","outputId":"5c465f2e-52cf-4c64-9743-cfc6bb5d3aad"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fa0be89b8f0>"]},"metadata":{},"execution_count":2}],"source":["import imageio\n","import torch\n","from PIL import Image\n","from torchvision import transforms\n","import collections\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torchvision import datasets\n","data_path = '../data-unversioned/p1ch7/'\n","torch.set_printoptions(edgeitems=2)\n","torch.manual_seed(123)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1648665200505,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"},"user_tz":240},"id":"dXR9MI9r9unO","outputId":"fb141944-bb49-43de-813b-57069e63a87c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":3}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('cpu')\n","\n","device"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"oT_7KPjJ9uBi","colab":{"base_uri":"https://localhost:8080/","height":122,"referenced_widgets":["b46c110ab99647cbb0d54b172e1f5f52","7c578e227d654c70a841f46b08881c5a","92e1ace7c79f4c619f311eef71f383c4","80c77ad358cd43869ef908b0daf37292","8b829b6cb33943f4b1e6cd5d0514a958","aff27db74d6442578637dbe7e124e47d","c0d18a3c21d34bb197729f04ecfa96ea","a809c516a02449d79956d84755e02590","887bf1b12d0644c1a726aa66b133517b","e088ab8ecb424a4dba5544b04940f0fa","a6947ee658274be39957504c92d6037b"]},"executionInfo":{"status":"ok","timestamp":1648665208102,"user_tz":240,"elapsed":7608,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}},"outputId":"d9033e26-987a-4681-c5c7-fb0fabd8f5e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b46c110ab99647cbb0d54b172e1f5f52"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ../data-unversioned/p1ch7/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/\n","Files already downloaded and verified\n"]}],"source":["cifar10 = datasets.CIFAR10(\n","    data_path, train=True, download=True,\n","\n","transform=transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4915,0.4823,0.4468),\n","                          (0.2470,0.2435,0.2616))\n","]))\n","\n","cifar10_val = datasets.CIFAR10(\n","    data_path, train=False, download=True,\n","\n","transform=transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4915,0.4823,0.4468),\n","                          (0.2470,0.2435,0.2616))\n","]))"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"7A_sVCe9CkAv","executionInfo":{"status":"ok","timestamp":1648665208103,"user_tz":240,"elapsed":13,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}}},"outputs":[],"source":["import datetime\n","def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n","  for epoch in range(1, n_epochs + 1):\n","    loss_train = 0.0\n","    for imgs, labels in train_loader:\n","      imgs = imgs.to(device=device)\n","      labels = labels.to(device=device)\n","      outputs = model(imgs)\n","      loss = loss_fn(outputs, labels)\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      loss_train += loss.item()\n","    print('{} Epoch {}, Training loss {}'.format(\n","    datetime.datetime.now(), epoch,\n","    loss_train / len(train_loader)))"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ZD8exYjvB34o","executionInfo":{"status":"ok","timestamp":1648665208103,"user_tz":240,"elapsed":12,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}}},"outputs":[],"source":["def validate(model, train_loader, val_loader):\n","  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","      for imgs, labels in loader:\n","          imgs, labels = imgs.to(device), labels.to(device)\n","          batch_size = imgs.shape[0]\n","          outputs = model(imgs)\n","          _, predicted = torch.max(outputs, dim=1)\n","          total += labels.shape[0]\n","          correct += int((predicted == labels).sum())\n","    print(\"Accuracy {}: {:.2f}\".format(name , correct / total))"]},{"cell_type":"code","source":["##############################################################################\n","############################## Problem 1 Part 1 ##############################\n","##############################################################################"],"metadata":{"id":"e2vWYbfAp0gn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0prPSM8a982w"},"outputs":[],"source":["class Net(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n","    self.act1 = nn.Tanh()\n","    self.pool1 = nn.MaxPool2d(2)\n","    self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n","    self.act2 = nn.Tanh()\n","    self.pool2 = nn.MaxPool2d(2)\n","    self.fc1 = nn.Linear(8 * 8 * 8, 32)\n","    self.act3 = nn.Tanh()\n","    self.fc2 = nn.Linear(32, 10)\n","\n","  def forward(self, x):\n","    out = self.pool1(self.act1(self.conv1(x)))\n","    out = self.pool2(self.act2(self.conv2(out)))\n","    out = out.view(-1, 8 * 8 * 8)\n","    out = self.act3(self.fc1(out))\n","    out = self.fc2(out)\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ThzHpbXz-dYJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"36519451-99b0-4d85-c97d-2cb3d67287e8","executionInfo":{"status":"ok","timestamp":1648565589138,"user_tz":240,"elapsed":2877439,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2022-03-29 14:05:30.011782 Epoch 1, Training loss 2.02721984261442\n","2022-03-29 14:05:40.087972 Epoch 2, Training loss 1.7497853926380578\n","2022-03-29 14:05:50.107808 Epoch 3, Training loss 1.5772642540504864\n","2022-03-29 14:05:59.863208 Epoch 4, Training loss 1.4792075047407613\n","2022-03-29 14:06:09.793509 Epoch 5, Training loss 1.4073018411846112\n","2022-03-29 14:06:19.359287 Epoch 6, Training loss 1.3408892368111769\n","2022-03-29 14:06:28.925203 Epoch 7, Training loss 1.2817198612805827\n","2022-03-29 14:06:38.259927 Epoch 8, Training loss 1.2311831129633861\n","2022-03-29 14:06:48.023221 Epoch 9, Training loss 1.1906220897689195\n","2022-03-29 14:06:57.645081 Epoch 10, Training loss 1.157154878691944\n","2022-03-29 14:07:07.445828 Epoch 11, Training loss 1.128880636313992\n","2022-03-29 14:07:17.228615 Epoch 12, Training loss 1.1048525478833777\n","2022-03-29 14:07:26.969562 Epoch 13, Training loss 1.0835032950124472\n","2022-03-29 14:07:36.476041 Epoch 14, Training loss 1.0672177808059147\n","2022-03-29 14:07:46.004019 Epoch 15, Training loss 1.04934817796473\n","2022-03-29 14:07:55.544784 Epoch 16, Training loss 1.034454861260436\n","2022-03-29 14:08:04.940683 Epoch 17, Training loss 1.0206575313644946\n","2022-03-29 14:08:14.520838 Epoch 18, Training loss 1.0089530062949872\n","2022-03-29 14:08:24.162720 Epoch 19, Training loss 0.9971860381953247\n","2022-03-29 14:08:33.656949 Epoch 20, Training loss 0.9850157194430261\n","2022-03-29 14:08:43.363807 Epoch 21, Training loss 0.9753733062378281\n","2022-03-29 14:08:52.965878 Epoch 22, Training loss 0.9660607514631413\n","2022-03-29 14:09:02.711967 Epoch 23, Training loss 0.9582381349085541\n","2022-03-29 14:09:12.300785 Epoch 24, Training loss 0.9501572754376989\n","2022-03-29 14:09:23.147647 Epoch 25, Training loss 0.9404567951893867\n","2022-03-29 14:09:32.707803 Epoch 26, Training loss 0.9335793404627943\n","2022-03-29 14:09:42.520513 Epoch 27, Training loss 0.9269013660185782\n","2022-03-29 14:09:52.174960 Epoch 28, Training loss 0.9177816344801423\n","2022-03-29 14:10:01.719956 Epoch 29, Training loss 0.9120591967307088\n","2022-03-29 14:10:11.296335 Epoch 30, Training loss 0.90462101702495\n","2022-03-29 14:10:21.061908 Epoch 31, Training loss 0.8995769715979886\n","2022-03-29 14:10:30.827648 Epoch 32, Training loss 0.8924614206299453\n","2022-03-29 14:10:40.211928 Epoch 33, Training loss 0.8877138767553412\n","2022-03-29 14:10:50.023230 Epoch 34, Training loss 0.8805860834353415\n","2022-03-29 14:10:59.652134 Epoch 35, Training loss 0.8762867307419058\n","2022-03-29 14:11:09.133850 Epoch 36, Training loss 0.8702653804627221\n","2022-03-29 14:11:18.618313 Epoch 37, Training loss 0.86650260116743\n","2022-03-29 14:11:28.190348 Epoch 38, Training loss 0.8604431861196943\n","2022-03-29 14:11:37.738167 Epoch 39, Training loss 0.8558145662402863\n","2022-03-29 14:11:47.435513 Epoch 40, Training loss 0.8509496586859379\n","2022-03-29 14:11:56.919751 Epoch 41, Training loss 0.8461926097760115\n","2022-03-29 14:12:06.802109 Epoch 42, Training loss 0.8425203565974979\n","2022-03-29 14:12:16.664661 Epoch 43, Training loss 0.8374654237571579\n","2022-03-29 14:12:26.467265 Epoch 44, Training loss 0.8330891888464809\n","2022-03-29 14:12:36.088629 Epoch 45, Training loss 0.8282545732567682\n","2022-03-29 14:12:45.793873 Epoch 46, Training loss 0.823358953989985\n","2022-03-29 14:12:55.343179 Epoch 47, Training loss 0.8204686962582571\n","2022-03-29 14:13:04.800668 Epoch 48, Training loss 0.8161729904620544\n","2022-03-29 14:13:14.517900 Epoch 49, Training loss 0.8127785442811449\n","2022-03-29 14:13:23.845813 Epoch 50, Training loss 0.8095821811796149\n","2022-03-29 14:13:33.341138 Epoch 51, Training loss 0.8059391770368952\n","2022-03-29 14:13:43.118709 Epoch 52, Training loss 0.8025984184059036\n","2022-03-29 14:13:52.947557 Epoch 53, Training loss 0.798363714571804\n","2022-03-29 14:14:02.720094 Epoch 54, Training loss 0.7961019107600307\n","2022-03-29 14:14:12.759138 Epoch 55, Training loss 0.7927937507629395\n","2022-03-29 14:14:23.794957 Epoch 56, Training loss 0.7867475916128939\n","2022-03-29 14:14:33.389981 Epoch 57, Training loss 0.7838872564799341\n","2022-03-29 14:14:42.856344 Epoch 58, Training loss 0.78163231608203\n","2022-03-29 14:14:52.482095 Epoch 59, Training loss 0.7783536333638383\n","2022-03-29 14:15:02.219951 Epoch 60, Training loss 0.7747723925525271\n","2022-03-29 14:15:11.914208 Epoch 61, Training loss 0.7734279963366516\n","2022-03-29 14:15:21.369111 Epoch 62, Training loss 0.7702645919359553\n","2022-03-29 14:15:31.092047 Epoch 63, Training loss 0.7669606284259836\n","2022-03-29 14:15:40.814123 Epoch 64, Training loss 0.7637795885200696\n","2022-03-29 14:15:50.543585 Epoch 65, Training loss 0.761945018073177\n","2022-03-29 14:15:59.945263 Epoch 66, Training loss 0.7586647690562032\n","2022-03-29 14:16:09.451187 Epoch 67, Training loss 0.7566452192147369\n","2022-03-29 14:16:19.263325 Epoch 68, Training loss 0.7531021440120609\n","2022-03-29 14:16:28.823522 Epoch 69, Training loss 0.7513717410661985\n","2022-03-29 14:16:38.892025 Epoch 70, Training loss 0.7476750630170793\n","2022-03-29 14:16:48.601230 Epoch 71, Training loss 0.7482643783702265\n","2022-03-29 14:16:58.582513 Epoch 72, Training loss 0.7432189740412071\n","2022-03-29 14:17:08.274741 Epoch 73, Training loss 0.7399483775086415\n","2022-03-29 14:17:17.742849 Epoch 74, Training loss 0.7388626013113104\n","2022-03-29 14:17:27.118195 Epoch 75, Training loss 0.7345785875911908\n","2022-03-29 14:17:36.837851 Epoch 76, Training loss 0.73500477215823\n","2022-03-29 14:17:46.532419 Epoch 77, Training loss 0.730902255724763\n","2022-03-29 14:17:55.990009 Epoch 78, Training loss 0.7286904858963569\n","2022-03-29 14:18:05.782738 Epoch 79, Training loss 0.7266480212321367\n","2022-03-29 14:18:15.367041 Epoch 80, Training loss 0.7258389530050785\n","2022-03-29 14:18:25.010484 Epoch 81, Training loss 0.721423318357114\n","2022-03-29 14:18:34.847358 Epoch 82, Training loss 0.7185242371562192\n","2022-03-29 14:18:44.119047 Epoch 83, Training loss 0.7176825522690478\n","2022-03-29 14:18:53.467292 Epoch 84, Training loss 0.7147198478355432\n","2022-03-29 14:19:03.047401 Epoch 85, Training loss 0.7132912035031087\n","2022-03-29 14:19:12.927905 Epoch 86, Training loss 0.7114184393602259\n","2022-03-29 14:19:22.944933 Epoch 87, Training loss 0.7087655889865992\n","2022-03-29 14:19:32.616881 Epoch 88, Training loss 0.7045451750230911\n","2022-03-29 14:19:42.019941 Epoch 89, Training loss 0.7071223794804204\n","2022-03-29 14:19:51.557105 Epoch 90, Training loss 0.7057135442791083\n","2022-03-29 14:20:01.114004 Epoch 91, Training loss 0.7011224871401287\n","2022-03-29 14:20:10.458293 Epoch 92, Training loss 0.7003407639539455\n","2022-03-29 14:20:19.930772 Epoch 93, Training loss 0.6986929156133891\n","2022-03-29 14:20:29.385810 Epoch 94, Training loss 0.6952922078197264\n","2022-03-29 14:20:39.153907 Epoch 95, Training loss 0.6949509435030811\n","2022-03-29 14:20:48.602137 Epoch 96, Training loss 0.69105899250111\n","2022-03-29 14:20:58.211630 Epoch 97, Training loss 0.6921093071360722\n","2022-03-29 14:21:07.694223 Epoch 98, Training loss 0.6886719104350375\n","2022-03-29 14:21:17.226721 Epoch 99, Training loss 0.686551994679834\n","2022-03-29 14:21:26.666244 Epoch 100, Training loss 0.6838261817422364\n","2022-03-29 14:21:36.293426 Epoch 101, Training loss 0.6825183758802731\n","2022-03-29 14:21:47.275317 Epoch 102, Training loss 0.6835416813793085\n","2022-03-29 14:21:58.212834 Epoch 103, Training loss 0.6795132347689871\n","2022-03-29 14:22:07.897037 Epoch 104, Training loss 0.6784043334938986\n","2022-03-29 14:22:17.469431 Epoch 105, Training loss 0.676389561742163\n","2022-03-29 14:22:26.940385 Epoch 106, Training loss 0.6768997417920081\n","2022-03-29 14:22:36.598267 Epoch 107, Training loss 0.672603397113283\n","2022-03-29 14:22:46.058562 Epoch 108, Training loss 0.6727710766789249\n","2022-03-29 14:22:56.846066 Epoch 109, Training loss 0.6701041594948001\n","2022-03-29 14:23:06.286285 Epoch 110, Training loss 0.6697211034996126\n","2022-03-29 14:23:15.728621 Epoch 111, Training loss 0.66721389170193\n","2022-03-29 14:23:25.428650 Epoch 112, Training loss 0.6659230735829419\n","2022-03-29 14:23:35.781642 Epoch 113, Training loss 0.6634193544878679\n","2022-03-29 14:23:45.490518 Epoch 114, Training loss 0.6623211147458962\n","2022-03-29 14:23:55.037708 Epoch 115, Training loss 0.6610246863968842\n","2022-03-29 14:24:05.059955 Epoch 116, Training loss 0.660854588803428\n","2022-03-29 14:24:15.180665 Epoch 117, Training loss 0.6582354103665218\n","2022-03-29 14:24:24.776490 Epoch 118, Training loss 0.6574815759802108\n","2022-03-29 14:24:34.336755 Epoch 119, Training loss 0.6549997610585464\n","2022-03-29 14:24:43.642211 Epoch 120, Training loss 0.6543008701880569\n","2022-03-29 14:24:53.456036 Epoch 121, Training loss 0.6518671202766316\n","2022-03-29 14:25:02.883085 Epoch 122, Training loss 0.6515132555037814\n","2022-03-29 14:25:12.300790 Epoch 123, Training loss 0.6493602823418425\n","2022-03-29 14:25:22.236192 Epoch 124, Training loss 0.6481871583577618\n","2022-03-29 14:25:31.430285 Epoch 125, Training loss 0.6459263670627419\n","2022-03-29 14:25:40.781370 Epoch 126, Training loss 0.6446015102707822\n","2022-03-29 14:25:50.054628 Epoch 127, Training loss 0.6445426777805514\n","2022-03-29 14:25:59.633531 Epoch 128, Training loss 0.6429189818213358\n","2022-03-29 14:26:09.421438 Epoch 129, Training loss 0.6405027066274067\n","2022-03-29 14:26:18.765317 Epoch 130, Training loss 0.6393215892183811\n","2022-03-29 14:26:28.304125 Epoch 131, Training loss 0.6381634737905639\n","2022-03-29 14:26:37.795653 Epoch 132, Training loss 0.6366987510029313\n","2022-03-29 14:26:46.966679 Epoch 133, Training loss 0.6359325206416953\n","2022-03-29 14:26:56.408844 Epoch 134, Training loss 0.6343360251325476\n","2022-03-29 14:27:06.085181 Epoch 135, Training loss 0.6332538865335152\n","2022-03-29 14:27:15.602915 Epoch 136, Training loss 0.6313781133469414\n","2022-03-29 14:27:24.997932 Epoch 137, Training loss 0.6319588340456833\n","2022-03-29 14:27:34.377284 Epoch 138, Training loss 0.6283353928790982\n","2022-03-29 14:27:43.848179 Epoch 139, Training loss 0.6294793778139612\n","2022-03-29 14:27:53.302436 Epoch 140, Training loss 0.6264596603181966\n","2022-03-29 14:28:02.661320 Epoch 141, Training loss 0.6257358701408976\n","2022-03-29 14:28:11.846634 Epoch 142, Training loss 0.6241385401286128\n","2022-03-29 14:28:21.149080 Epoch 143, Training loss 0.6246192274267411\n","2022-03-29 14:28:30.478757 Epoch 144, Training loss 0.620833880418097\n","2022-03-29 14:28:39.868180 Epoch 145, Training loss 0.6209096506123652\n","2022-03-29 14:28:49.564596 Epoch 146, Training loss 0.6208972836775548\n","2022-03-29 14:28:59.076685 Epoch 147, Training loss 0.6189742329556619\n","2022-03-29 14:29:08.558540 Epoch 148, Training loss 0.617780799679744\n","2022-03-29 14:29:17.971618 Epoch 149, Training loss 0.6158235732018186\n","2022-03-29 14:29:27.126202 Epoch 150, Training loss 0.6163038354548042\n","2022-03-29 14:29:36.348762 Epoch 151, Training loss 0.6147234948623516\n","2022-03-29 14:29:46.051221 Epoch 152, Training loss 0.6126699639708185\n","2022-03-29 14:29:55.484596 Epoch 153, Training loss 0.613057017326355\n","2022-03-29 14:30:04.715372 Epoch 154, Training loss 0.6112702279673208\n","2022-03-29 14:30:13.980334 Epoch 155, Training loss 0.608900640664808\n","2022-03-29 14:30:23.254783 Epoch 156, Training loss 0.6076839650835832\n","2022-03-29 14:30:32.586129 Epoch 157, Training loss 0.606143363067866\n","2022-03-29 14:30:41.976704 Epoch 158, Training loss 0.6065191005349464\n","2022-03-29 14:30:51.142182 Epoch 159, Training loss 0.6059855878200677\n","2022-03-29 14:31:00.529374 Epoch 160, Training loss 0.6032079047025622\n","2022-03-29 14:31:09.758093 Epoch 161, Training loss 0.6023365590730896\n","2022-03-29 14:31:18.996214 Epoch 162, Training loss 0.6008475957738469\n","2022-03-29 14:31:28.148207 Epoch 163, Training loss 0.6007378862794402\n","2022-03-29 14:31:37.711970 Epoch 164, Training loss 0.6004619400976868\n","2022-03-29 14:31:47.105535 Epoch 165, Training loss 0.5995980725645105\n","2022-03-29 14:31:56.531467 Epoch 166, Training loss 0.5987655092840609\n","2022-03-29 14:32:05.940499 Epoch 167, Training loss 0.5974084487199174\n","2022-03-29 14:32:15.088522 Epoch 168, Training loss 0.5956985403585922\n","2022-03-29 14:32:24.721181 Epoch 169, Training loss 0.5953033554660695\n","2022-03-29 14:32:34.146335 Epoch 170, Training loss 0.5933790772467318\n","2022-03-29 14:32:43.470811 Epoch 171, Training loss 0.5940725011441409\n","2022-03-29 14:32:52.883634 Epoch 172, Training loss 0.5918753270602897\n","2022-03-29 14:33:02.265409 Epoch 173, Training loss 0.5916295826739972\n","2022-03-29 14:33:11.743047 Epoch 174, Training loss 0.5900289520354527\n","2022-03-29 14:33:21.245450 Epoch 175, Training loss 0.58879936080607\n","2022-03-29 14:33:30.511292 Epoch 176, Training loss 0.5885987740267268\n","2022-03-29 14:33:39.792543 Epoch 177, Training loss 0.586856328198672\n","2022-03-29 14:33:49.081179 Epoch 178, Training loss 0.587873802465551\n","2022-03-29 14:33:58.396527 Epoch 179, Training loss 0.586434990586832\n","2022-03-29 14:34:07.840331 Epoch 180, Training loss 0.5863427621934115\n","2022-03-29 14:34:17.315944 Epoch 181, Training loss 0.5848204344129928\n","2022-03-29 14:34:26.786824 Epoch 182, Training loss 0.582725941334539\n","2022-03-29 14:34:36.196357 Epoch 183, Training loss 0.5827386804172755\n","2022-03-29 14:34:45.729294 Epoch 184, Training loss 0.5818288935267407\n","2022-03-29 14:34:54.883578 Epoch 185, Training loss 0.5812502696614741\n","2022-03-29 14:35:04.124033 Epoch 186, Training loss 0.5811253490350435\n","2022-03-29 14:35:13.496364 Epoch 187, Training loss 0.5766139818198236\n","2022-03-29 14:35:22.792039 Epoch 188, Training loss 0.5778824526178258\n","2022-03-29 14:35:32.484044 Epoch 189, Training loss 0.5772033281567152\n","2022-03-29 14:35:41.986838 Epoch 190, Training loss 0.5769924340040787\n","2022-03-29 14:35:51.540863 Epoch 191, Training loss 0.5757302150244603\n","2022-03-29 14:36:01.064077 Epoch 192, Training loss 0.5736382472545595\n","2022-03-29 14:36:10.504976 Epoch 193, Training loss 0.5747956606509436\n","2022-03-29 14:36:19.684950 Epoch 194, Training loss 0.5734320911757477\n","2022-03-29 14:36:29.105509 Epoch 195, Training loss 0.5738458952025685\n","2022-03-29 14:36:38.496665 Epoch 196, Training loss 0.5712381418784866\n","2022-03-29 14:36:47.954101 Epoch 197, Training loss 0.5719194909190888\n","2022-03-29 14:36:57.454155 Epoch 198, Training loss 0.5705046621734834\n","2022-03-29 14:37:06.967128 Epoch 199, Training loss 0.5700481013035226\n","2022-03-29 14:37:16.505883 Epoch 200, Training loss 0.5673322396738755\n","2022-03-29 14:37:25.753158 Epoch 201, Training loss 0.5679337744365263\n","2022-03-29 14:37:34.952856 Epoch 202, Training loss 0.5665362581343907\n","2022-03-29 14:37:44.492630 Epoch 203, Training loss 0.5647866808240066\n","2022-03-29 14:37:53.984114 Epoch 204, Training loss 0.56615072115303\n","2022-03-29 14:38:03.476714 Epoch 205, Training loss 0.5640292288854604\n","2022-03-29 14:38:12.948045 Epoch 206, Training loss 0.5646748866342828\n","2022-03-29 14:38:22.242335 Epoch 207, Training loss 0.5640471898343252\n","2022-03-29 14:38:31.664882 Epoch 208, Training loss 0.5632037593199469\n","2022-03-29 14:38:41.229696 Epoch 209, Training loss 0.5629595400351087\n","2022-03-29 14:38:50.714778 Epoch 210, Training loss 0.5612189211237156\n","2022-03-29 14:39:00.030087 Epoch 211, Training loss 0.5608017475098905\n","2022-03-29 14:39:09.611325 Epoch 212, Training loss 0.5588135178894034\n","2022-03-29 14:39:19.149798 Epoch 213, Training loss 0.5597798579260517\n","2022-03-29 14:39:28.875044 Epoch 214, Training loss 0.559328005518145\n","2022-03-29 14:39:38.390114 Epoch 215, Training loss 0.5606788702099524\n","2022-03-29 14:39:48.083073 Epoch 216, Training loss 0.5566134994749523\n","2022-03-29 14:39:57.600978 Epoch 217, Training loss 0.5563678114158114\n","2022-03-29 14:40:07.177893 Epoch 218, Training loss 0.5562030290017652\n","2022-03-29 14:40:16.437084 Epoch 219, Training loss 0.5562580511774249\n","2022-03-29 14:40:25.837027 Epoch 220, Training loss 0.5559633366020439\n","2022-03-29 14:40:35.307049 Epoch 221, Training loss 0.5543410819585975\n","2022-03-29 14:40:44.672014 Epoch 222, Training loss 0.5529820815757718\n","2022-03-29 14:40:54.227918 Epoch 223, Training loss 0.5540046769258616\n","2022-03-29 14:41:03.626567 Epoch 224, Training loss 0.550953051508845\n","2022-03-29 14:41:13.173647 Epoch 225, Training loss 0.550799894058491\n","2022-03-29 14:41:22.757088 Epoch 226, Training loss 0.5540038872405392\n","2022-03-29 14:41:32.230323 Epoch 227, Training loss 0.5512877677560157\n","2022-03-29 14:41:41.521741 Epoch 228, Training loss 0.5512126322711826\n","2022-03-29 14:41:50.955020 Epoch 229, Training loss 0.5491347254046699\n","2022-03-29 14:42:00.331215 Epoch 230, Training loss 0.5493448103404106\n","2022-03-29 14:42:09.780239 Epoch 231, Training loss 0.5486235753883182\n","2022-03-29 14:42:19.322107 Epoch 232, Training loss 0.5476467330437487\n","2022-03-29 14:42:28.761356 Epoch 233, Training loss 0.5470588053278911\n","2022-03-29 14:42:38.169745 Epoch 234, Training loss 0.5478994348622344\n","2022-03-29 14:42:47.610881 Epoch 235, Training loss 0.5455527886024216\n","2022-03-29 14:42:56.851355 Epoch 236, Training loss 0.544856211032404\n","2022-03-29 14:43:06.251107 Epoch 237, Training loss 0.5430389105740105\n","2022-03-29 14:43:15.808901 Epoch 238, Training loss 0.5432983452401807\n","2022-03-29 14:43:25.438254 Epoch 239, Training loss 0.5422977399262016\n","2022-03-29 14:43:35.002670 Epoch 240, Training loss 0.5430200404256506\n","2022-03-29 14:43:44.527907 Epoch 241, Training loss 0.5395899601185413\n","2022-03-29 14:43:54.048858 Epoch 242, Training loss 0.5420688445991872\n","2022-03-29 14:44:03.747665 Epoch 243, Training loss 0.5408038787372277\n","2022-03-29 14:44:13.375776 Epoch 244, Training loss 0.540021529568888\n","2022-03-29 14:44:22.525985 Epoch 245, Training loss 0.5405836649563002\n","2022-03-29 14:44:32.027993 Epoch 246, Training loss 0.5423444236635857\n","2022-03-29 14:44:41.531119 Epoch 247, Training loss 0.5393748887816964\n","2022-03-29 14:44:51.076786 Epoch 248, Training loss 0.5398254030577058\n","2022-03-29 14:45:00.636860 Epoch 249, Training loss 0.5402756325538506\n","2022-03-29 14:45:10.429653 Epoch 250, Training loss 0.5387830735777345\n","2022-03-29 14:45:20.070007 Epoch 251, Training loss 0.5371882666826553\n","2022-03-29 14:45:29.525637 Epoch 252, Training loss 0.5362447824167169\n","2022-03-29 14:45:38.747715 Epoch 253, Training loss 0.5345774714065634\n","2022-03-29 14:45:48.427178 Epoch 254, Training loss 0.5353410117080449\n","2022-03-29 14:45:58.030145 Epoch 255, Training loss 0.5341418433524764\n","2022-03-29 14:46:07.714759 Epoch 256, Training loss 0.5348371235687105\n","2022-03-29 14:46:17.276337 Epoch 257, Training loss 0.5345737051857097\n","2022-03-29 14:46:26.683121 Epoch 258, Training loss 0.5340803523578912\n","2022-03-29 14:46:36.350842 Epoch 259, Training loss 0.5327633302230055\n","2022-03-29 14:46:45.936211 Epoch 260, Training loss 0.53120312151854\n","2022-03-29 14:46:55.385070 Epoch 261, Training loss 0.5322344142495824\n","2022-03-29 14:47:04.681337 Epoch 262, Training loss 0.5299493979157694\n","2022-03-29 14:47:14.307197 Epoch 263, Training loss 0.5314116102960104\n","2022-03-29 14:47:23.764604 Epoch 264, Training loss 0.5297025042154905\n","2022-03-29 14:47:33.307149 Epoch 265, Training loss 0.529532087962036\n","2022-03-29 14:47:42.875422 Epoch 266, Training loss 0.5290687029700145\n","2022-03-29 14:47:52.526916 Epoch 267, Training loss 0.532254588363878\n","2022-03-29 14:48:02.060839 Epoch 268, Training loss 0.530164363953616\n","2022-03-29 14:48:11.464427 Epoch 269, Training loss 0.5280188823409397\n","2022-03-29 14:48:20.729200 Epoch 270, Training loss 0.5292716825099857\n","2022-03-29 14:48:30.534798 Epoch 271, Training loss 0.5277351880317454\n","2022-03-29 14:48:40.115222 Epoch 272, Training loss 0.5264450951534159\n","2022-03-29 14:48:49.713351 Epoch 273, Training loss 0.5272087713946467\n","2022-03-29 14:48:59.185362 Epoch 274, Training loss 0.5271789398797028\n","2022-03-29 14:49:09.057256 Epoch 275, Training loss 0.5236227960728318\n","2022-03-29 14:49:18.529196 Epoch 276, Training loss 0.5249157989459574\n","2022-03-29 14:49:28.156559 Epoch 277, Training loss 0.5256266418434775\n","2022-03-29 14:49:37.592897 Epoch 278, Training loss 0.5250459826358443\n","2022-03-29 14:49:47.154643 Epoch 279, Training loss 0.5244597814729451\n","2022-03-29 14:49:56.897072 Epoch 280, Training loss 0.5234486668958993\n","2022-03-29 14:50:06.550805 Epoch 281, Training loss 0.5229720504540006\n","2022-03-29 14:50:16.296983 Epoch 282, Training loss 0.5229886269859035\n","2022-03-29 14:50:25.837608 Epoch 283, Training loss 0.5222542109658651\n","2022-03-29 14:50:35.322100 Epoch 284, Training loss 0.5226814403482105\n","2022-03-29 14:50:44.910998 Epoch 285, Training loss 0.5224657816731412\n","2022-03-29 14:50:54.492183 Epoch 286, Training loss 0.520731788233418\n","2022-03-29 14:51:03.802662 Epoch 287, Training loss 0.5197684378613292\n","2022-03-29 14:51:13.412086 Epoch 288, Training loss 0.5201317934543276\n","2022-03-29 14:51:23.029877 Epoch 289, Training loss 0.5189630981830075\n","2022-03-29 14:51:32.538042 Epoch 290, Training loss 0.5205849513335301\n","2022-03-29 14:51:41.852922 Epoch 291, Training loss 0.5171283902147846\n","2022-03-29 14:51:51.339012 Epoch 292, Training loss 0.5195075798674923\n","2022-03-29 14:52:01.152348 Epoch 293, Training loss 0.5164103289241986\n","2022-03-29 14:52:10.844735 Epoch 294, Training loss 0.5183748188226119\n","2022-03-29 14:52:20.280191 Epoch 295, Training loss 0.5185682057114818\n","2022-03-29 14:52:29.691373 Epoch 296, Training loss 0.5177411213326637\n","2022-03-29 14:52:39.169183 Epoch 297, Training loss 0.5164581584884688\n","2022-03-29 14:52:48.946471 Epoch 298, Training loss 0.5176775983303709\n","2022-03-29 14:52:58.962524 Epoch 299, Training loss 0.5150456811918322\n","2022-03-29 14:53:08.623946 Epoch 300, Training loss 0.5135455200129457\n"]}],"source":["train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n","                                          shuffle=True)\n","model = Net().to(device=device)\n","optimizer = optim.SGD(model.parameters(), lr=1e-2)\n","loss_fn = nn.CrossEntropyLoss()\n","training_loop(\n","  n_epochs = 300,\n","  optimizer = optimizer,\n","  model = model,\n","  loss_fn = loss_fn,\n","  train_loader = train_loader,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuDRamk-GFix","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648565599583,"user_tz":240,"elapsed":10448,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}},"outputId":"530e8016-d8d8-4d9e-8e9b-68d506bf9abb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy train: 0.81\n","Accuracy val: 0.62\n"]}],"source":["train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n","                                          shuffle=False)\n","val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n","                                        shuffle=False)\n","validate(model, train_loader, val_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PjZkV948r7Nt"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P_qqZyLAtP42"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSHwGwn8tQAT"},"outputs":[],"source":["##############################################################################\n","############################## Problem 1 Part 2 ##############################\n","##############################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91TySA5DtQH2"},"outputs":[],"source":["class Net2(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n","    self.act1 = nn.Tanh()\n","    self.pool1 = nn.MaxPool2d(2)\n","    self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n","    self.act2 = nn.Tanh()\n","    self.pool2 = nn.MaxPool2d(2)\n","    self.conv3 = nn.Conv2d(8, 4, kernel_size=3, padding=1)\n","    self.act3 = nn.Tanh()\n","    self.pool3 = nn.MaxPool2d(2)\n","    self.fc1 = nn.Linear(4 * 4 * 4, 32)\n","    self.act3 = nn.Tanh()\n","    self.fc2 = nn.Linear(32, 10)\n","\n","  def forward(self, x):\n","    out = self.pool1(self.act1(self.conv1(x)))\n","    out = self.pool2(self.act2(self.conv2(out)))\n","    out = self.pool3(self.act3(self.conv3(out)))\n","    out = out.view(-1, 4 * 4 * 4)\n","    out = self.act3(self.fc1(out))\n","    out = self.fc2(out)\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Up53uDqKuF_S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648569195035,"user_tz":240,"elapsed":1317045,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}},"outputId":"233e87bf-646b-499b-9966-a4a767505382"},"outputs":[{"output_type":"stream","name":"stdout","text":["2022-03-29 15:04:07.458859 Epoch 1, Training loss 2.1719703678889655\n","2022-03-29 15:04:17.448987 Epoch 2, Training loss 1.9653784640304877\n","2022-03-29 15:04:27.206111 Epoch 3, Training loss 1.8669427848228104\n","2022-03-29 15:04:37.033651 Epoch 4, Training loss 1.7318873138683837\n","2022-03-29 15:04:47.020100 Epoch 5, Training loss 1.6289830893811668\n","2022-03-29 15:04:56.900992 Epoch 6, Training loss 1.5667944687711612\n","2022-03-29 15:05:06.923919 Epoch 7, Training loss 1.5080518550275233\n","2022-03-29 15:05:16.907148 Epoch 8, Training loss 1.4609321005204146\n","2022-03-29 15:05:26.959439 Epoch 9, Training loss 1.420656356512738\n","2022-03-29 15:05:36.942796 Epoch 10, Training loss 1.390820705143692\n","2022-03-29 15:05:46.687221 Epoch 11, Training loss 1.366698873012572\n","2022-03-29 15:05:56.513970 Epoch 12, Training loss 1.3431210291507605\n","2022-03-29 15:06:06.577681 Epoch 13, Training loss 1.323814850100471\n","2022-03-29 15:06:16.688604 Epoch 14, Training loss 1.3040657347577918\n","2022-03-29 15:06:26.639626 Epoch 15, Training loss 1.2897293345092813\n","2022-03-29 15:06:36.482973 Epoch 16, Training loss 1.2750699653497437\n","2022-03-29 15:06:46.199574 Epoch 17, Training loss 1.2613123544799092\n","2022-03-29 15:06:56.194172 Epoch 18, Training loss 1.2488199427457112\n","2022-03-29 15:07:06.109256 Epoch 19, Training loss 1.2383683265170173\n","2022-03-29 15:07:15.725271 Epoch 20, Training loss 1.225314557399896\n","2022-03-29 15:07:25.600292 Epoch 21, Training loss 1.2151089179546326\n","2022-03-29 15:07:35.558100 Epoch 22, Training loss 1.2055720518464628\n","2022-03-29 15:07:45.172162 Epoch 23, Training loss 1.197266775933678\n","2022-03-29 15:07:55.241870 Epoch 24, Training loss 1.1874311028234183\n","2022-03-29 15:08:05.020316 Epoch 25, Training loss 1.1773678904299236\n","2022-03-29 15:08:14.926094 Epoch 26, Training loss 1.1675786517770088\n","2022-03-29 15:08:24.599977 Epoch 27, Training loss 1.159921656865293\n","2022-03-29 15:08:34.201738 Epoch 28, Training loss 1.1537763335363334\n","2022-03-29 15:08:44.041641 Epoch 29, Training loss 1.146363869957302\n","2022-03-29 15:08:53.910827 Epoch 30, Training loss 1.138278759882578\n","2022-03-29 15:09:03.691160 Epoch 31, Training loss 1.1308981306717525\n","2022-03-29 15:09:13.662389 Epoch 32, Training loss 1.1277175269773245\n","2022-03-29 15:09:23.691557 Epoch 33, Training loss 1.1190439074698006\n","2022-03-29 15:09:33.922083 Epoch 34, Training loss 1.1127672216776388\n","2022-03-29 15:09:43.866190 Epoch 35, Training loss 1.108043033219969\n","2022-03-29 15:09:53.580839 Epoch 36, Training loss 1.1020902842664353\n","2022-03-29 15:10:03.561923 Epoch 37, Training loss 1.097096741351935\n","2022-03-29 15:10:13.288430 Epoch 38, Training loss 1.0928755143414373\n","2022-03-29 15:10:23.530647 Epoch 39, Training loss 1.0868709521068027\n","2022-03-29 15:10:33.373434 Epoch 40, Training loss 1.0851512927838298\n","2022-03-29 15:10:43.424191 Epoch 41, Training loss 1.0796395033369284\n","2022-03-29 15:10:53.231582 Epoch 42, Training loss 1.0765981955448989\n","2022-03-29 15:11:03.121798 Epoch 43, Training loss 1.0722908679481662\n","2022-03-29 15:11:12.841592 Epoch 44, Training loss 1.067545774266543\n","2022-03-29 15:11:22.907548 Epoch 45, Training loss 1.0656703073350364\n","2022-03-29 15:11:32.641228 Epoch 46, Training loss 1.0610071524329807\n","2022-03-29 15:11:42.471669 Epoch 47, Training loss 1.0587515805840797\n","2022-03-29 15:11:52.313140 Epoch 48, Training loss 1.0566988038589886\n","2022-03-29 15:12:02.483171 Epoch 49, Training loss 1.054656065257309\n","2022-03-29 15:12:12.245559 Epoch 50, Training loss 1.048204587243707\n","2022-03-29 15:12:22.353380 Epoch 51, Training loss 1.0489907209068308\n","2022-03-29 15:12:32.166589 Epoch 52, Training loss 1.0431347711921652\n","2022-03-29 15:12:41.736197 Epoch 53, Training loss 1.0428114436623994\n","2022-03-29 15:12:51.766877 Epoch 54, Training loss 1.0372176291540152\n","2022-03-29 15:13:01.833917 Epoch 55, Training loss 1.0353604851628813\n","2022-03-29 15:13:12.000361 Epoch 56, Training loss 1.036817063227334\n","2022-03-29 15:13:22.445999 Epoch 57, Training loss 1.0324500459996635\n","2022-03-29 15:13:32.308637 Epoch 58, Training loss 1.0279486691555404\n","2022-03-29 15:13:42.313167 Epoch 59, Training loss 1.0279568279033426\n","2022-03-29 15:13:52.137536 Epoch 60, Training loss 1.0241847887368458\n","2022-03-29 15:14:02.046586 Epoch 61, Training loss 1.02296508341799\n","2022-03-29 15:14:12.124326 Epoch 62, Training loss 1.020340435492718\n","2022-03-29 15:14:22.041063 Epoch 63, Training loss 1.016903985491799\n","2022-03-29 15:14:32.021734 Epoch 64, Training loss 1.0169626402732965\n","2022-03-29 15:14:42.064589 Epoch 65, Training loss 1.0149053448758771\n","2022-03-29 15:14:51.684947 Epoch 66, Training loss 1.0135862283084704\n","2022-03-29 15:15:01.547132 Epoch 67, Training loss 1.011474246869002\n","2022-03-29 15:15:11.085672 Epoch 68, Training loss 1.0093851666468794\n","2022-03-29 15:15:20.786339 Epoch 69, Training loss 1.0069792225690144\n","2022-03-29 15:15:30.646063 Epoch 70, Training loss 1.0060263344698854\n","2022-03-29 15:15:40.536574 Epoch 71, Training loss 1.004803338776464\n","2022-03-29 15:15:50.364629 Epoch 72, Training loss 1.00205563081195\n","2022-03-29 15:16:00.454238 Epoch 73, Training loss 1.0015333260569121\n","2022-03-29 15:16:10.620166 Epoch 74, Training loss 1.000720511007187\n","2022-03-29 15:16:20.857941 Epoch 75, Training loss 0.998712311162973\n","2022-03-29 15:16:30.843114 Epoch 76, Training loss 0.9996809063817534\n","2022-03-29 15:16:40.509384 Epoch 77, Training loss 0.9971760830001148\n","2022-03-29 15:16:50.548416 Epoch 78, Training loss 0.9946702609739035\n","2022-03-29 15:17:00.728098 Epoch 79, Training loss 0.9937779556607347\n","2022-03-29 15:17:10.847913 Epoch 80, Training loss 0.992880627932146\n","2022-03-29 15:17:20.982575 Epoch 81, Training loss 0.9890319865073085\n","2022-03-29 15:17:31.029072 Epoch 82, Training loss 0.9888227167336837\n","2022-03-29 15:17:40.953013 Epoch 83, Training loss 0.9881773552931178\n","2022-03-29 15:17:50.819661 Epoch 84, Training loss 0.9875730326413499\n","2022-03-29 15:18:00.500691 Epoch 85, Training loss 0.986045041581249\n","2022-03-29 15:18:10.493420 Epoch 86, Training loss 0.9852681156924313\n","2022-03-29 15:18:20.549945 Epoch 87, Training loss 0.9849920593716605\n","2022-03-29 15:18:30.381085 Epoch 88, Training loss 0.981859717451398\n","2022-03-29 15:18:40.373824 Epoch 89, Training loss 0.9799047761865894\n","2022-03-29 15:18:50.583080 Epoch 90, Training loss 0.9786433753607523\n","2022-03-29 15:19:00.649533 Epoch 91, Training loss 0.9796025677562674\n","2022-03-29 15:19:10.551421 Epoch 92, Training loss 0.9791241626605354\n","2022-03-29 15:19:20.122945 Epoch 93, Training loss 0.9772982800098331\n","2022-03-29 15:19:29.994073 Epoch 94, Training loss 0.9749967090766448\n","2022-03-29 15:19:40.024347 Epoch 95, Training loss 0.9734762184455267\n","2022-03-29 15:19:50.049959 Epoch 96, Training loss 0.9735194453802865\n","2022-03-29 15:19:59.991233 Epoch 97, Training loss 0.9723944133504883\n","2022-03-29 15:20:10.014805 Epoch 98, Training loss 0.9714963506249821\n","2022-03-29 15:20:19.995836 Epoch 99, Training loss 0.9699067892625813\n","2022-03-29 15:20:30.245695 Epoch 100, Training loss 0.9709033527032799\n","2022-03-29 15:20:39.858768 Epoch 101, Training loss 0.9670718370953484\n","2022-03-29 15:20:49.791021 Epoch 102, Training loss 0.9670173104309365\n","2022-03-29 15:20:59.825807 Epoch 103, Training loss 0.9656729153202622\n","2022-03-29 15:21:09.943869 Epoch 104, Training loss 0.9643277657763732\n","2022-03-29 15:21:19.958133 Epoch 105, Training loss 0.9646959650089674\n","2022-03-29 15:21:29.997518 Epoch 106, Training loss 0.964240110438803\n","2022-03-29 15:21:39.944496 Epoch 107, Training loss 0.9627370626267875\n","2022-03-29 15:21:49.901801 Epoch 108, Training loss 0.9592369736155586\n","2022-03-29 15:21:59.496994 Epoch 109, Training loss 0.9623903335665193\n","2022-03-29 15:22:09.342982 Epoch 110, Training loss 0.9594337523288434\n","2022-03-29 15:22:19.375779 Epoch 111, Training loss 0.9599597134706005\n","2022-03-29 15:22:29.443237 Epoch 112, Training loss 0.9588410583755854\n","2022-03-29 15:22:39.286312 Epoch 113, Training loss 0.9564322075423073\n","2022-03-29 15:22:49.529427 Epoch 114, Training loss 0.9573224111438712\n","2022-03-29 15:22:59.437743 Epoch 115, Training loss 0.9543202443958243\n","2022-03-29 15:23:09.289560 Epoch 116, Training loss 0.9555349835501913\n","2022-03-29 15:23:18.890012 Epoch 117, Training loss 0.9553542622672323\n","2022-03-29 15:23:28.646010 Epoch 118, Training loss 0.9558268952400178\n","2022-03-29 15:23:38.698270 Epoch 119, Training loss 0.9512345770283428\n","2022-03-29 15:23:48.599025 Epoch 120, Training loss 0.9515809720129613\n","2022-03-29 15:23:58.695920 Epoch 121, Training loss 0.950670132094332\n","2022-03-29 15:24:08.572119 Epoch 122, Training loss 0.9492038964005687\n","2022-03-29 15:24:18.639942 Epoch 123, Training loss 0.9504204541063674\n","2022-03-29 15:24:28.627560 Epoch 124, Training loss 0.9492389005620766\n","2022-03-29 15:24:38.306309 Epoch 125, Training loss 0.9478680524984588\n","2022-03-29 15:24:48.116763 Epoch 126, Training loss 0.9469542248017343\n","2022-03-29 15:24:57.778063 Epoch 127, Training loss 0.9478235978757024\n","2022-03-29 15:25:07.681073 Epoch 128, Training loss 0.9458144589915605\n","2022-03-29 15:25:17.553674 Epoch 129, Training loss 0.9453657821316244\n","2022-03-29 15:25:27.313177 Epoch 130, Training loss 0.9437085666010142\n","2022-03-29 15:25:37.037194 Epoch 131, Training loss 0.9441410514247387\n","2022-03-29 15:25:46.745392 Epoch 132, Training loss 0.9427268937268221\n","2022-03-29 15:25:56.497408 Epoch 133, Training loss 0.9433869846031794\n","2022-03-29 15:26:06.153166 Epoch 134, Training loss 0.9422194764894598\n","2022-03-29 15:26:15.922226 Epoch 135, Training loss 0.9402076684300552\n","2022-03-29 15:26:25.903588 Epoch 136, Training loss 0.9422340736822095\n","2022-03-29 15:26:35.727353 Epoch 137, Training loss 0.9414714796616294\n","2022-03-29 15:26:45.484270 Epoch 138, Training loss 0.9413884853767922\n","2022-03-29 15:26:55.476312 Epoch 139, Training loss 0.9387084471295252\n","2022-03-29 15:27:05.394222 Epoch 140, Training loss 0.9379796955134253\n","2022-03-29 15:27:15.160140 Epoch 141, Training loss 0.9365232910036736\n","2022-03-29 15:27:24.603875 Epoch 142, Training loss 0.9365329865909293\n","2022-03-29 15:27:34.240160 Epoch 143, Training loss 0.9371128746920534\n","2022-03-29 15:27:44.331786 Epoch 144, Training loss 0.9356387693558812\n","2022-03-29 15:27:54.139136 Epoch 145, Training loss 0.9360774924688022\n","2022-03-29 15:28:03.922485 Epoch 146, Training loss 0.9340330227409177\n","2022-03-29 15:28:13.720507 Epoch 147, Training loss 0.9361154702313416\n","2022-03-29 15:28:23.413487 Epoch 148, Training loss 0.9334110497208812\n","2022-03-29 15:28:33.237527 Epoch 149, Training loss 0.9335016397868886\n","2022-03-29 15:28:42.775965 Epoch 150, Training loss 0.9316932180196124\n","2022-03-29 15:28:52.515512 Epoch 151, Training loss 0.931922482178949\n","2022-03-29 15:29:02.794545 Epoch 152, Training loss 0.9313103623707276\n","2022-03-29 15:29:12.522518 Epoch 153, Training loss 0.9302846665882394\n","2022-03-29 15:29:22.256435 Epoch 154, Training loss 0.9295799734494875\n","2022-03-29 15:29:32.178378 Epoch 155, Training loss 0.9305374354428952\n","2022-03-29 15:29:42.310903 Epoch 156, Training loss 0.9306339689380373\n","2022-03-29 15:29:52.341237 Epoch 157, Training loss 0.9297309512143854\n","2022-03-29 15:30:02.026177 Epoch 158, Training loss 0.9274635396497634\n","2022-03-29 15:30:11.750349 Epoch 159, Training loss 0.9273877223891676\n","2022-03-29 15:30:21.799646 Epoch 160, Training loss 0.9283658276738413\n","2022-03-29 15:30:31.511013 Epoch 161, Training loss 0.9280229302318505\n","2022-03-29 15:30:41.375314 Epoch 162, Training loss 0.9264166125708528\n","2022-03-29 15:30:51.155335 Epoch 163, Training loss 0.9264501935380804\n","2022-03-29 15:31:00.975936 Epoch 164, Training loss 0.9248783395570868\n","2022-03-29 15:31:10.694385 Epoch 165, Training loss 0.9253829500406904\n","2022-03-29 15:31:20.576981 Epoch 166, Training loss 0.9250796607235814\n","2022-03-29 15:31:30.123862 Epoch 167, Training loss 0.9232407850225258\n","2022-03-29 15:31:39.759278 Epoch 168, Training loss 0.9241437641403559\n","2022-03-29 15:31:49.621382 Epoch 169, Training loss 0.9236060278799832\n","2022-03-29 15:31:59.554695 Epoch 170, Training loss 0.9224461978658691\n","2022-03-29 15:32:09.409593 Epoch 171, Training loss 0.9225287742322058\n","2022-03-29 15:32:19.301288 Epoch 172, Training loss 0.9230426719883824\n","2022-03-29 15:32:29.088277 Epoch 173, Training loss 0.9207558029752863\n","2022-03-29 15:32:38.987443 Epoch 174, Training loss 0.9210193481896539\n","2022-03-29 15:32:48.439495 Epoch 175, Training loss 0.919047228668047\n","2022-03-29 15:32:58.183505 Epoch 176, Training loss 0.9205961338699321\n","2022-03-29 15:33:08.113060 Epoch 177, Training loss 0.9194213895084303\n","2022-03-29 15:33:17.897475 Epoch 178, Training loss 0.9195096997348854\n","2022-03-29 15:33:27.681016 Epoch 179, Training loss 0.9189579223885256\n","2022-03-29 15:33:37.605796 Epoch 180, Training loss 0.9203681788023781\n","2022-03-29 15:33:47.394130 Epoch 181, Training loss 0.9195897736970116\n","2022-03-29 15:33:57.117169 Epoch 182, Training loss 0.9173484186992011\n","2022-03-29 15:34:06.627749 Epoch 183, Training loss 0.9179682860441525\n","2022-03-29 15:34:16.317135 Epoch 184, Training loss 0.9174019796464145\n","2022-03-29 15:34:26.033194 Epoch 185, Training loss 0.9165489280315311\n","2022-03-29 15:34:35.796646 Epoch 186, Training loss 0.9173145679104359\n","2022-03-29 15:34:45.503408 Epoch 187, Training loss 0.914163810441561\n","2022-03-29 15:34:55.494609 Epoch 188, Training loss 0.9169608417069516\n","2022-03-29 15:35:05.248870 Epoch 189, Training loss 0.9147884654419501\n","2022-03-29 15:35:14.834815 Epoch 190, Training loss 0.9140477874089995\n","2022-03-29 15:35:24.562497 Epoch 191, Training loss 0.9163465547134809\n","2022-03-29 15:35:34.110444 Epoch 192, Training loss 0.914317308103337\n","2022-03-29 15:35:43.895295 Epoch 193, Training loss 0.9141468520054732\n","2022-03-29 15:35:53.521355 Epoch 194, Training loss 0.9132758378220336\n","2022-03-29 15:36:03.485172 Epoch 195, Training loss 0.9132962364827275\n","2022-03-29 15:36:13.561970 Epoch 196, Training loss 0.9138561889643559\n","2022-03-29 15:36:23.390867 Epoch 197, Training loss 0.9117054789877304\n","2022-03-29 15:36:33.201853 Epoch 198, Training loss 0.9126783258012493\n","2022-03-29 15:36:42.795256 Epoch 199, Training loss 0.9107804697035523\n","2022-03-29 15:36:52.430026 Epoch 200, Training loss 0.9113457769231723\n","2022-03-29 15:37:02.532685 Epoch 201, Training loss 0.9114711564748793\n","2022-03-29 15:37:12.404315 Epoch 202, Training loss 0.9094512323894159\n","2022-03-29 15:37:22.308801 Epoch 203, Training loss 0.9093128383312079\n","2022-03-29 15:37:32.037585 Epoch 204, Training loss 0.9096830277644155\n","2022-03-29 15:37:41.703093 Epoch 205, Training loss 0.9087311277913925\n","2022-03-29 15:37:51.518778 Epoch 206, Training loss 0.9073853589918303\n","2022-03-29 15:38:01.550563 Epoch 207, Training loss 0.9092566391543659\n","2022-03-29 15:38:10.983316 Epoch 208, Training loss 0.9076936763265858\n","2022-03-29 15:38:20.982593 Epoch 209, Training loss 0.908135702085617\n","2022-03-29 15:38:30.781827 Epoch 210, Training loss 0.9077892970398563\n","2022-03-29 15:38:40.523367 Epoch 211, Training loss 0.9078021964148792\n","2022-03-29 15:38:50.552677 Epoch 212, Training loss 0.9067913582715232\n","2022-03-29 15:39:00.587938 Epoch 213, Training loss 0.9055116942623997\n","2022-03-29 15:39:10.495754 Epoch 214, Training loss 0.9062905842843263\n","2022-03-29 15:39:20.428468 Epoch 215, Training loss 0.9054642260227057\n","2022-03-29 15:39:30.051019 Epoch 216, Training loss 0.9046192522091634\n","2022-03-29 15:39:39.814583 Epoch 217, Training loss 0.9045307111480961\n","2022-03-29 15:39:49.682435 Epoch 218, Training loss 0.9051503338624755\n","2022-03-29 15:39:59.518270 Epoch 219, Training loss 0.9048823794288099\n","2022-03-29 15:40:09.326675 Epoch 220, Training loss 0.9031439003584635\n","2022-03-29 15:40:19.246710 Epoch 221, Training loss 0.903573581934585\n","2022-03-29 15:40:29.095368 Epoch 222, Training loss 0.9048829465113637\n","2022-03-29 15:40:38.902243 Epoch 223, Training loss 0.9033868927937334\n","2022-03-29 15:40:48.548405 Epoch 224, Training loss 0.90293203176135\n","2022-03-29 15:40:58.313004 Epoch 225, Training loss 0.901293348549577\n","2022-03-29 15:41:08.049923 Epoch 226, Training loss 0.9026186856467401\n","2022-03-29 15:41:17.931069 Epoch 227, Training loss 0.9035388767109502\n","2022-03-29 15:41:27.733313 Epoch 228, Training loss 0.9023724133553712\n","2022-03-29 15:41:37.735033 Epoch 229, Training loss 0.9007623827518405\n","2022-03-29 15:41:47.619376 Epoch 230, Training loss 0.9005248850721228\n","2022-03-29 15:41:57.469976 Epoch 231, Training loss 0.9011597259880026\n","2022-03-29 15:42:07.087622 Epoch 232, Training loss 0.9024540514439878\n","2022-03-29 15:42:16.669019 Epoch 233, Training loss 0.9014658618461141\n","2022-03-29 15:42:26.703329 Epoch 234, Training loss 0.900933010498886\n","2022-03-29 15:42:36.569697 Epoch 235, Training loss 0.8996095147431659\n","2022-03-29 15:42:46.557667 Epoch 236, Training loss 0.8985344246982614\n","2022-03-29 15:42:56.166041 Epoch 237, Training loss 0.8986246052300534\n","2022-03-29 15:43:06.341904 Epoch 238, Training loss 0.8993717877151411\n","2022-03-29 15:43:16.304798 Epoch 239, Training loss 0.8988349340151033\n","2022-03-29 15:43:26.222435 Epoch 240, Training loss 0.8985447702962724\n","2022-03-29 15:43:35.815225 Epoch 241, Training loss 0.8974647908411977\n","2022-03-29 15:43:45.705090 Epoch 242, Training loss 0.8990865326141153\n","2022-03-29 15:43:55.602505 Epoch 243, Training loss 0.8976371351562803\n","2022-03-29 15:44:05.568693 Epoch 244, Training loss 0.8977451999016735\n","2022-03-29 15:44:15.534646 Epoch 245, Training loss 0.8981591750441305\n","2022-03-29 15:44:25.286846 Epoch 246, Training loss 0.8976975917968604\n","2022-03-29 15:44:35.090184 Epoch 247, Training loss 0.8969545724904141\n","2022-03-29 15:44:44.949782 Epoch 248, Training loss 0.8957967436527048\n","2022-03-29 15:44:54.473523 Epoch 249, Training loss 0.8968872483581534\n","2022-03-29 15:45:04.572023 Epoch 250, Training loss 0.8953053359790226\n","2022-03-29 15:45:14.586799 Epoch 251, Training loss 0.8958309370919567\n","2022-03-29 15:45:24.387722 Epoch 252, Training loss 0.8934669645546037\n","2022-03-29 15:45:34.270140 Epoch 253, Training loss 0.8965636282930594\n","2022-03-29 15:45:44.148712 Epoch 254, Training loss 0.8952095578698551\n","2022-03-29 15:45:53.875356 Epoch 255, Training loss 0.8947233010817062\n","2022-03-29 15:46:03.755864 Epoch 256, Training loss 0.8939446279459902\n","2022-03-29 15:46:13.267636 Epoch 257, Training loss 0.8955390774990286\n","2022-03-29 15:46:22.837678 Epoch 258, Training loss 0.8947042868570294\n","2022-03-29 15:46:32.401244 Epoch 259, Training loss 0.8927966962232614\n","2022-03-29 15:46:42.110489 Epoch 260, Training loss 0.892504174195592\n","2022-03-29 15:46:52.048700 Epoch 261, Training loss 0.8931515269419726\n","2022-03-29 15:47:01.984528 Epoch 262, Training loss 0.8948350664599777\n","2022-03-29 15:47:11.686796 Epoch 263, Training loss 0.8945454977205037\n","2022-03-29 15:47:21.541598 Epoch 264, Training loss 0.8919834007921121\n","2022-03-29 15:47:31.126051 Epoch 265, Training loss 0.8930287776548235\n","2022-03-29 15:47:40.815123 Epoch 266, Training loss 0.8923637642503699\n","2022-03-29 15:47:50.844352 Epoch 267, Training loss 0.8922093014430512\n","2022-03-29 15:48:00.664420 Epoch 268, Training loss 0.891313638185601\n","2022-03-29 15:48:10.401441 Epoch 269, Training loss 0.8906490667854123\n","2022-03-29 15:48:20.117361 Epoch 270, Training loss 0.8918539348160824\n","2022-03-29 15:48:29.944086 Epoch 271, Training loss 0.8907018244418952\n","2022-03-29 15:48:39.935431 Epoch 272, Training loss 0.8903422691023258\n","2022-03-29 15:48:49.648006 Epoch 273, Training loss 0.8908261760421421\n","2022-03-29 15:48:59.217561 Epoch 274, Training loss 0.8910851415313418\n","2022-03-29 15:49:09.015772 Epoch 275, Training loss 0.8911365812544323\n","2022-03-29 15:49:18.673209 Epoch 276, Training loss 0.8902419727019337\n","2022-03-29 15:49:28.764397 Epoch 277, Training loss 0.8900986523045908\n","2022-03-29 15:49:38.626264 Epoch 278, Training loss 0.8890554757069444\n","2022-03-29 15:49:48.427192 Epoch 279, Training loss 0.8891630118993847\n","2022-03-29 15:49:58.227691 Epoch 280, Training loss 0.8893456412550739\n","2022-03-29 15:50:08.035598 Epoch 281, Training loss 0.8891776994518612\n","2022-03-29 15:50:17.539065 Epoch 282, Training loss 0.887688355677573\n","2022-03-29 15:50:27.296335 Epoch 283, Training loss 0.8901898979835803\n","2022-03-29 15:50:37.149105 Epoch 284, Training loss 0.8867485260262209\n","2022-03-29 15:50:46.981753 Epoch 285, Training loss 0.8889096399097491\n","2022-03-29 15:50:56.602610 Epoch 286, Training loss 0.8881912738313456\n","2022-03-29 15:51:06.503947 Epoch 287, Training loss 0.8869588564881279\n","2022-03-29 15:51:16.551978 Epoch 288, Training loss 0.8879311638110129\n","2022-03-29 15:51:26.571987 Epoch 289, Training loss 0.8884317015138123\n","2022-03-29 15:51:36.056056 Epoch 290, Training loss 0.8878099598619335\n","2022-03-29 15:51:45.857963 Epoch 291, Training loss 0.8862582586153084\n","2022-03-29 15:51:55.662111 Epoch 292, Training loss 0.8862298788774349\n","2022-03-29 15:52:05.561150 Epoch 293, Training loss 0.8873846998909856\n","2022-03-29 15:52:15.570670 Epoch 294, Training loss 0.8862137063537412\n","2022-03-29 15:52:25.654746 Epoch 295, Training loss 0.8850557350594065\n","2022-03-29 15:52:35.509607 Epoch 296, Training loss 0.8863589118051407\n","2022-03-29 15:52:45.292922 Epoch 297, Training loss 0.8852608453129869\n","2022-03-29 15:52:55.037112 Epoch 298, Training loss 0.8853333012374771\n","2022-03-29 15:53:04.749743 Epoch 299, Training loss 0.8847623516988876\n","2022-03-29 15:53:14.651922 Epoch 300, Training loss 0.8832852553071269\n"]}],"source":["train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n","                                          shuffle=True)\n","model2 = Net2().to(device=device)\n","optimizer = optim.SGD(model2.parameters(), lr=1e-2)\n","loss_fn = nn.CrossEntropyLoss()\n","training_loop(\n","  n_epochs = 300,\n","  optimizer = optimizer,\n","  model = model2,\n","  loss_fn = loss_fn,\n","  train_loader = train_loader,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uvEvXR994zKA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648569206015,"user_tz":240,"elapsed":10981,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}},"outputId":"7cd8962c-22f4-4e9e-f0f9-6dfb918610ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy train: 0.68\n","Accuracy val: 0.64\n"]}],"source":["train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n","                                          shuffle=False)\n","val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n","                                        shuffle=False)\n","validate(model2, train_loader, val_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m7vb2RxlLdf8"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"syXAknD7LdnN"},"outputs":[],"source":[""]},{"cell_type":"code","source":["###############################################################################\n","################################## Problem 2 ##################################\n","###############################################################################"],"metadata":{"id":"oghU7OwFp9vx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUFJgAJaLdq9"},"outputs":[],"source":["###############################################################################\n","################################## ResNet-10 ##################################\n","###############################################################################"]},{"cell_type":"code","source":["all_acc_dict = collections.OrderedDict()"],"metadata":{"id":"JbX4f2PaB5dJ","executionInfo":{"status":"ok","timestamp":1648665599045,"user_tz":240,"elapsed":231,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class ResBlock(nn.Module):\n","    def __init__(self, n_chans):\n","        super(ResBlock, self).__init__()\n","        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n","                              padding=1, bias=False)  # <1>\n","        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n","        torch.nn.init.kaiming_normal_(self.conv.weight,\n","                                      nonlinearity='relu')  # <2>\n","        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n","        torch.nn.init.zeros_(self.batch_norm.bias)\n","\n","    def forward(self, x):\n","        out = self.conv(x)\n","        out = self.batch_norm(out)\n","        out = torch.relu(out)\n","        return out + x"],"metadata":{"id":"UjkSJ3EFrEtc","executionInfo":{"status":"ok","timestamp":1648665602647,"user_tz":240,"elapsed":218,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class NetResDeep(nn.Module):\n","    def __init__(self, n_chans1=32, n_blocks=10):\n","        super().__init__()\n","        self.n_chans1 = n_chans1\n","        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n","        self.resblocks = nn.Sequential(\n","            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n","        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n","        self.fc2 = nn.Linear(32, 10)\n","        \n","    def forward(self, x):\n","        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n","        out = self.resblocks(out)\n","        out = F.max_pool2d(out, 2)\n","        out = out.view(-1, 8 * 8 * self.n_chans1)\n","        out = torch.relu(self.fc1(out))\n","        out = self.fc2(out)\n","        return out"],"metadata":{"id":"f4xS4vp7oXNa","executionInfo":{"status":"ok","timestamp":1648665603843,"user_tz":240,"elapsed":166,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n","                                          shuffle=True)\n","model3 = NetResDeep(n_chans1=32, n_blocks=10).to(device=device)\n","optimizer = optim.SGD(model3.parameters(), lr=3e-3)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","training_loop(\n","    n_epochs = 300,\n","    optimizer = optimizer,\n","    model = model3,\n","    loss_fn = loss_fn,\n","    train_loader = train_loader,\n",")"],"metadata":{"id":"LHXUx-d-oZaX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648595539933,"user_tz":240,"elapsed":5634576,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}},"outputId":"e4c40d3b-a74b-4081-d7c4-05165a3b0068"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-03-29 21:38:44.193684 Epoch 1, Training loss 1.658337791252624\n","2022-03-29 21:39:02.534176 Epoch 2, Training loss 1.3571581625572555\n","2022-03-29 21:39:20.699279 Epoch 3, Training loss 1.2195725649823923\n","2022-03-29 21:39:39.476839 Epoch 4, Training loss 1.1272219763997267\n","2022-03-29 21:39:58.298072 Epoch 5, Training loss 1.061702290459362\n","2022-03-29 21:40:17.279771 Epoch 6, Training loss 1.0111957640599107\n","2022-03-29 21:40:36.270593 Epoch 7, Training loss 0.9623677023231526\n","2022-03-29 21:40:54.765563 Epoch 8, Training loss 0.923695315256753\n","2022-03-29 21:41:15.184420 Epoch 9, Training loss 0.890067790780226\n","2022-03-29 21:41:37.911143 Epoch 10, Training loss 0.862966438922126\n","2022-03-29 21:42:00.051882 Epoch 11, Training loss 0.8361911259953628\n","2022-03-29 21:42:18.858741 Epoch 12, Training loss 0.8104548426464085\n","2022-03-29 21:42:37.747499 Epoch 13, Training loss 0.7847263403332142\n","2022-03-29 21:42:56.658348 Epoch 14, Training loss 0.7658165612870165\n","2022-03-29 21:43:17.108037 Epoch 15, Training loss 0.7466959720453643\n","2022-03-29 21:43:36.020867 Epoch 16, Training loss 0.7289182127589156\n","2022-03-29 21:43:55.114753 Epoch 17, Training loss 0.7159188866920179\n","2022-03-29 21:44:14.315334 Epoch 18, Training loss 0.6958879625324703\n","2022-03-29 21:44:33.296762 Epoch 19, Training loss 0.6797312224078971\n","2022-03-29 21:44:52.110070 Epoch 20, Training loss 0.6641257963979336\n","2022-03-29 21:45:14.850802 Epoch 21, Training loss 0.6500024799724369\n","2022-03-29 21:45:34.898509 Epoch 22, Training loss 0.6380516278850453\n","2022-03-29 21:45:54.496267 Epoch 23, Training loss 0.6262577189432691\n","2022-03-29 21:46:15.369278 Epoch 24, Training loss 0.6086018649132355\n","2022-03-29 21:46:35.191997 Epoch 25, Training loss 0.594801055462769\n","2022-03-29 21:46:54.809893 Epoch 26, Training loss 0.5830511696960615\n","2022-03-29 21:47:14.068459 Epoch 27, Training loss 0.5720845578271715\n","2022-03-29 21:47:32.916118 Epoch 28, Training loss 0.564385415190626\n","2022-03-29 21:47:52.718966 Epoch 29, Training loss 0.5468207356112692\n","2022-03-29 21:48:11.740853 Epoch 30, Training loss 0.5377227824438563\n","2022-03-29 21:48:30.318024 Epoch 31, Training loss 0.5291626011319173\n","2022-03-29 21:48:48.977249 Epoch 32, Training loss 0.5162795952061559\n","2022-03-29 21:49:08.481412 Epoch 33, Training loss 0.5067097726837754\n","2022-03-29 21:49:27.668052 Epoch 34, Training loss 0.49284747117163274\n","2022-03-29 21:49:47.108372 Epoch 35, Training loss 0.48282417866503796\n","2022-03-29 21:50:06.195941 Epoch 36, Training loss 0.4730982504346791\n","2022-03-29 21:50:24.659097 Epoch 37, Training loss 0.46335273859141124\n","2022-03-29 21:50:43.479510 Epoch 38, Training loss 0.4561871169778087\n","2022-03-29 21:51:02.448051 Epoch 39, Training loss 0.4456355704752076\n","2022-03-29 21:51:21.037350 Epoch 40, Training loss 0.43737012462314134\n","2022-03-29 21:51:39.763339 Epoch 41, Training loss 0.4243172465268608\n","2022-03-29 21:51:58.424425 Epoch 42, Training loss 0.41650027074777257\n","2022-03-29 21:52:16.929799 Epoch 43, Training loss 0.41085723015810827\n","2022-03-29 21:52:35.497374 Epoch 44, Training loss 0.39989854936557045\n","2022-03-29 21:52:54.473787 Epoch 45, Training loss 0.3918026565476452\n","2022-03-29 21:53:14.194170 Epoch 46, Training loss 0.37918931141000267\n","2022-03-29 21:53:32.866552 Epoch 47, Training loss 0.3725978969727331\n","2022-03-29 21:53:51.925552 Epoch 48, Training loss 0.3704894522343145\n","2022-03-29 21:54:10.677842 Epoch 49, Training loss 0.358810346649812\n","2022-03-29 21:54:29.388104 Epoch 50, Training loss 0.3531314414518569\n","2022-03-29 21:54:48.307587 Epoch 51, Training loss 0.3438565086411393\n","2022-03-29 21:55:07.098096 Epoch 52, Training loss 0.33365382185524994\n","2022-03-29 21:55:25.919384 Epoch 53, Training loss 0.3237037556936674\n","2022-03-29 21:55:44.564028 Epoch 54, Training loss 0.3164095034456009\n","2022-03-29 21:56:02.869331 Epoch 55, Training loss 0.31446793843108367\n","2022-03-29 21:56:21.713114 Epoch 56, Training loss 0.30459756143105304\n","2022-03-29 21:56:40.788825 Epoch 57, Training loss 0.2990313960177362\n","2022-03-29 21:56:59.669254 Epoch 58, Training loss 0.29076616191650595\n","2022-03-29 21:57:18.431836 Epoch 59, Training loss 0.28141497964482476\n","2022-03-29 21:57:37.318668 Epoch 60, Training loss 0.2729424985263811\n","2022-03-29 21:57:56.173809 Epoch 61, Training loss 0.26872262934132307\n","2022-03-29 21:58:14.953481 Epoch 62, Training loss 0.26427649049197927\n","2022-03-29 21:58:33.622991 Epoch 63, Training loss 0.25853341360531196\n","2022-03-29 21:58:52.203702 Epoch 64, Training loss 0.25045438492885025\n","2022-03-29 21:59:10.843139 Epoch 65, Training loss 0.2476020654487183\n","2022-03-29 21:59:29.517409 Epoch 66, Training loss 0.23764469455498868\n","2022-03-29 21:59:48.198401 Epoch 67, Training loss 0.2347289641218646\n","2022-03-29 22:00:06.570255 Epoch 68, Training loss 0.22738830280273467\n","2022-03-29 22:00:25.352348 Epoch 69, Training loss 0.22574084278796336\n","2022-03-29 22:00:44.038928 Epoch 70, Training loss 0.21742603544364958\n","2022-03-29 22:01:02.675907 Epoch 71, Training loss 0.2110255960079715\n","2022-03-29 22:01:21.123633 Epoch 72, Training loss 0.20581668297595837\n","2022-03-29 22:01:39.789193 Epoch 73, Training loss 0.2047934721859977\n","2022-03-29 22:01:58.307360 Epoch 74, Training loss 0.19231656705365158\n","2022-03-29 22:02:17.179165 Epoch 75, Training loss 0.1873425211271514\n","2022-03-29 22:02:35.794778 Epoch 76, Training loss 0.19351088041749298\n","2022-03-29 22:02:54.273123 Epoch 77, Training loss 0.1827450443792831\n","2022-03-29 22:03:13.046293 Epoch 78, Training loss 0.18188724385769775\n","2022-03-29 22:03:31.652832 Epoch 79, Training loss 0.1831962918610219\n","2022-03-29 22:03:50.331150 Epoch 80, Training loss 0.17278684296495164\n","2022-03-29 22:04:08.711048 Epoch 81, Training loss 0.16805159207433462\n","2022-03-29 22:04:27.280929 Epoch 82, Training loss 0.16362207860726377\n","2022-03-29 22:04:46.018825 Epoch 83, Training loss 0.1651589526459003\n","2022-03-29 22:05:04.469420 Epoch 84, Training loss 0.1535806516304498\n","2022-03-29 22:05:23.013768 Epoch 85, Training loss 0.15297431065736677\n","2022-03-29 22:05:41.644225 Epoch 86, Training loss 0.14337483138474813\n","2022-03-29 22:06:00.366620 Epoch 87, Training loss 0.14063427193314218\n","2022-03-29 22:06:18.860918 Epoch 88, Training loss 0.1380279908228256\n","2022-03-29 22:06:37.366269 Epoch 89, Training loss 0.13032085754815728\n","2022-03-29 22:06:56.027313 Epoch 90, Training loss 0.12967152258290734\n","2022-03-29 22:07:14.736664 Epoch 91, Training loss 0.13708840482784887\n","2022-03-29 22:07:33.397231 Epoch 92, Training loss 0.1271998078045447\n","2022-03-29 22:07:52.188477 Epoch 93, Training loss 0.13172146973802762\n","2022-03-29 22:08:11.088020 Epoch 94, Training loss 0.1192198739175106\n","2022-03-29 22:08:29.821166 Epoch 95, Training loss 0.1491250528160797\n","2022-03-29 22:08:48.831203 Epoch 96, Training loss 0.11646492199143371\n","2022-03-29 22:09:07.589745 Epoch 97, Training loss 0.11831642862628489\n","2022-03-29 22:09:26.323425 Epoch 98, Training loss 0.1054168303254182\n","2022-03-29 22:09:44.517200 Epoch 99, Training loss 0.1197730216705014\n","2022-03-29 22:10:03.150725 Epoch 100, Training loss 0.19838136257579947\n","2022-03-29 22:10:22.044239 Epoch 101, Training loss 0.152254419472745\n","2022-03-29 22:10:40.625002 Epoch 102, Training loss 0.1474906345984191\n","2022-03-29 22:10:59.242069 Epoch 103, Training loss 0.15186704801219275\n","2022-03-29 22:11:17.702998 Epoch 104, Training loss 0.12799167640678719\n","2022-03-29 22:11:36.602527 Epoch 105, Training loss 0.12544036534664882\n","2022-03-29 22:11:55.055530 Epoch 106, Training loss 0.1118435784674171\n","2022-03-29 22:12:13.845189 Epoch 107, Training loss 0.12084160249470674\n","2022-03-29 22:12:32.397013 Epoch 108, Training loss 0.11438022834746658\n","2022-03-29 22:12:51.015388 Epoch 109, Training loss 0.10720008619896629\n","2022-03-29 22:13:09.926795 Epoch 110, Training loss 0.10219085375156702\n","2022-03-29 22:13:28.774433 Epoch 111, Training loss 0.09284353832883374\n","2022-03-29 22:13:47.610583 Epoch 112, Training loss 0.13781381828014921\n","2022-03-29 22:14:07.616373 Epoch 113, Training loss 0.10001492322496403\n","2022-03-29 22:14:26.388263 Epoch 114, Training loss 0.08789485127753233\n","2022-03-29 22:14:45.158476 Epoch 115, Training loss 0.08073354192921306\n","2022-03-29 22:15:03.920422 Epoch 116, Training loss 0.08526047119213377\n","2022-03-29 22:15:22.457696 Epoch 117, Training loss 0.0760897425636935\n","2022-03-29 22:15:41.268430 Epoch 118, Training loss 0.07427603000289072\n","2022-03-29 22:16:00.179003 Epoch 119, Training loss 0.06618950618049868\n","2022-03-29 22:16:18.812579 Epoch 120, Training loss 0.07421159493805998\n","2022-03-29 22:16:37.480849 Epoch 121, Training loss 0.07586857998955643\n","2022-03-29 22:16:56.328165 Epoch 122, Training loss 0.07612195903616374\n","2022-03-29 22:17:15.059207 Epoch 123, Training loss 0.07916773919759275\n","2022-03-29 22:17:33.788181 Epoch 124, Training loss 0.09382124155726465\n","2022-03-29 22:17:53.274778 Epoch 125, Training loss 0.09237951497592585\n","2022-03-29 22:18:12.070779 Epoch 126, Training loss 0.08210242602169095\n","2022-03-29 22:18:30.839029 Epoch 127, Training loss 0.07955597376550937\n","2022-03-29 22:18:49.596676 Epoch 128, Training loss 0.06906967004880195\n","2022-03-29 22:19:08.422942 Epoch 129, Training loss 0.10886104091587465\n","2022-03-29 22:19:26.895851 Epoch 130, Training loss 0.06100187756125922\n","2022-03-29 22:19:45.748460 Epoch 131, Training loss 0.08996001126297067\n","2022-03-29 22:20:04.308873 Epoch 132, Training loss 0.08179171886795279\n","2022-03-29 22:20:23.323240 Epoch 133, Training loss 0.0715780091213534\n","2022-03-29 22:20:42.237572 Epoch 134, Training loss 0.05439943199693833\n","2022-03-29 22:21:00.942427 Epoch 135, Training loss 0.057770223902833776\n","2022-03-29 22:21:19.861680 Epoch 136, Training loss 0.07489914539725046\n","2022-03-29 22:21:38.490829 Epoch 137, Training loss 0.060512063878914696\n","2022-03-29 22:21:56.832080 Epoch 138, Training loss 0.05086415653209895\n","2022-03-29 22:22:15.382143 Epoch 139, Training loss 0.06386379024271122\n","2022-03-29 22:22:33.953188 Epoch 140, Training loss 0.06908441554871686\n","2022-03-29 22:22:52.478983 Epoch 141, Training loss 0.06033352269615997\n","2022-03-29 22:23:11.225188 Epoch 142, Training loss 0.06427020916853414\n","2022-03-29 22:23:29.730445 Epoch 143, Training loss 0.06846915482355238\n","2022-03-29 22:23:47.922058 Epoch 144, Training loss 0.0641541915607479\n","2022-03-29 22:24:06.344714 Epoch 145, Training loss 0.06978623089059006\n","2022-03-29 22:24:25.070137 Epoch 146, Training loss 0.04964743012471882\n","2022-03-29 22:24:43.774098 Epoch 147, Training loss 0.04545457731239269\n","2022-03-29 22:25:01.885414 Epoch 148, Training loss 0.04519078921010989\n","2022-03-29 22:25:20.032452 Epoch 149, Training loss 0.06585519052410732\n","2022-03-29 22:25:38.809354 Epoch 150, Training loss 0.05293651914332405\n","2022-03-29 22:25:57.637919 Epoch 151, Training loss 0.03307847007695238\n","2022-03-29 22:26:16.326522 Epoch 152, Training loss 0.03341275258460189\n","2022-03-29 22:26:34.477308 Epoch 153, Training loss 0.029214117988346198\n","2022-03-29 22:26:52.853399 Epoch 154, Training loss 0.02647586173468443\n","2022-03-29 22:27:11.245895 Epoch 155, Training loss 0.039191696361121735\n","2022-03-29 22:27:29.735133 Epoch 156, Training loss 0.056203953579396886\n","2022-03-29 22:27:48.592620 Epoch 157, Training loss 0.060780480335218844\n","2022-03-29 22:28:07.086356 Epoch 158, Training loss 0.05781124693417774\n","2022-03-29 22:28:25.896855 Epoch 159, Training loss 0.07970469719087205\n","2022-03-29 22:28:44.805993 Epoch 160, Training loss 0.057444605527652896\n","2022-03-29 22:29:03.264660 Epoch 161, Training loss 0.07004735907515906\n","2022-03-29 22:29:21.313369 Epoch 162, Training loss 0.05077439890858834\n","2022-03-29 22:29:39.765152 Epoch 163, Training loss 0.04105437617353581\n","2022-03-29 22:29:58.308453 Epoch 164, Training loss 0.024340594575926185\n","2022-03-29 22:30:16.661103 Epoch 165, Training loss 0.01815829816175496\n","2022-03-29 22:30:34.752554 Epoch 166, Training loss 0.01646578949375931\n","2022-03-29 22:30:53.162615 Epoch 167, Training loss 0.013124046447511781\n","2022-03-29 22:31:11.638125 Epoch 168, Training loss 0.013098235991771531\n","2022-03-29 22:31:30.325552 Epoch 169, Training loss 0.012825788329075546\n","2022-03-29 22:31:48.576175 Epoch 170, Training loss 0.025579732937572756\n","2022-03-29 22:32:06.896853 Epoch 171, Training loss 0.01204456275170002\n","2022-03-29 22:32:25.087782 Epoch 172, Training loss 0.017763283065742933\n","2022-03-29 22:32:43.312373 Epoch 173, Training loss 0.025125369969832823\n","2022-03-29 22:33:01.727192 Epoch 174, Training loss 0.013476192542557816\n","2022-03-29 22:33:19.917118 Epoch 175, Training loss 0.012564390760493914\n","2022-03-29 22:33:38.570451 Epoch 176, Training loss 0.017981364021954292\n","2022-03-29 22:33:56.993082 Epoch 177, Training loss 0.0225399118739322\n","2022-03-29 22:34:15.658854 Epoch 178, Training loss 0.0257171824911807\n","2022-03-29 22:34:33.990534 Epoch 179, Training loss 0.022033556499947198\n","2022-03-29 22:34:52.758697 Epoch 180, Training loss 0.11800741074863426\n","2022-03-29 22:35:11.435749 Epoch 181, Training loss 0.23097932331687998\n","2022-03-29 22:35:30.247477 Epoch 182, Training loss 0.12879042406870134\n","2022-03-29 22:35:48.991129 Epoch 183, Training loss 0.07399961041480713\n","2022-03-29 22:36:07.399609 Epoch 184, Training loss 0.044997352639289424\n","2022-03-29 22:36:26.071351 Epoch 185, Training loss 0.03499041576607777\n","2022-03-29 22:36:44.870792 Epoch 186, Training loss 0.020864635358786073\n","2022-03-29 22:37:03.492796 Epoch 187, Training loss 0.015168180648034857\n","2022-03-29 22:37:22.449188 Epoch 188, Training loss 0.013281842713560456\n","2022-03-29 22:37:41.157310 Epoch 189, Training loss 0.01054048036252532\n","2022-03-29 22:38:00.141676 Epoch 190, Training loss 0.01013031836612212\n","2022-03-29 22:38:19.078324 Epoch 191, Training loss 0.00931050363415078\n","2022-03-29 22:38:37.859016 Epoch 192, Training loss 0.007800109538799295\n","2022-03-29 22:38:56.562527 Epoch 193, Training loss 0.008403095210392666\n","2022-03-29 22:39:15.568379 Epoch 194, Training loss 0.009013913505066954\n","2022-03-29 22:39:34.338831 Epoch 195, Training loss 0.006158604364357757\n","2022-03-29 22:39:53.053213 Epoch 196, Training loss 0.00702701313065334\n","2022-03-29 22:40:11.587449 Epoch 197, Training loss 0.008363994503128188\n","2022-03-29 22:40:30.345565 Epoch 198, Training loss 0.005519864969360439\n","2022-03-29 22:40:49.083377 Epoch 199, Training loss 0.0051702743985608595\n","2022-03-29 22:41:07.834256 Epoch 200, Training loss 0.0045423131276299\n","2022-03-29 22:41:26.551989 Epoch 201, Training loss 0.003942090207445876\n","2022-03-29 22:41:44.957864 Epoch 202, Training loss 0.003367265065908051\n","2022-03-29 22:42:03.807776 Epoch 203, Training loss 0.0037493744892302824\n","2022-03-29 22:42:22.504749 Epoch 204, Training loss 0.003249092710056782\n","2022-03-29 22:42:41.167468 Epoch 205, Training loss 0.003911433486401013\n","2022-03-29 22:42:59.792426 Epoch 206, Training loss 0.00458974012243234\n","2022-03-29 22:43:18.329915 Epoch 207, Training loss 0.00372997297441064\n","2022-03-29 22:43:37.394156 Epoch 208, Training loss 0.00349048463976942\n","2022-03-29 22:43:56.339658 Epoch 209, Training loss 0.005918027072012915\n","2022-03-29 22:44:14.905243 Epoch 210, Training loss 0.003843526588790083\n","2022-03-29 22:44:33.341906 Epoch 211, Training loss 0.0056056976162791585\n","2022-03-29 22:44:52.050255 Epoch 212, Training loss 0.003659485899450262\n","2022-03-29 22:45:10.682771 Epoch 213, Training loss 0.0040957869068199\n","2022-03-29 22:45:29.330765 Epoch 214, Training loss 0.004811791858188939\n","2022-03-29 22:45:47.741719 Epoch 215, Training loss 0.032498175793615004\n","2022-03-29 22:46:06.456651 Epoch 216, Training loss 0.18597092178693075\n","2022-03-29 22:46:24.920458 Epoch 217, Training loss 0.17839672586456645\n","2022-03-29 22:46:43.516299 Epoch 218, Training loss 0.14739737988692586\n","2022-03-29 22:47:02.283488 Epoch 219, Training loss 0.08409797646643599\n","2022-03-29 22:47:20.888141 Epoch 220, Training loss 0.05625366083452897\n","2022-03-29 22:47:39.700428 Epoch 221, Training loss 0.03480502545117232\n","2022-03-29 22:47:58.543469 Epoch 222, Training loss 0.033549239763411245\n","2022-03-29 22:48:17.207202 Epoch 223, Training loss 0.03204736288930492\n","2022-03-29 22:48:35.876614 Epoch 224, Training loss 0.016554710547090807\n","2022-03-29 22:48:54.776110 Epoch 225, Training loss 0.010859450389011203\n","2022-03-29 22:49:13.662083 Epoch 226, Training loss 0.0065326330448051825\n","2022-03-29 22:49:32.505278 Epoch 227, Training loss 0.008448702556824268\n","2022-03-29 22:49:51.068902 Epoch 228, Training loss 0.006131432569504458\n","2022-03-29 22:50:09.565490 Epoch 229, Training loss 0.005676854038512801\n","2022-03-29 22:50:28.253342 Epoch 230, Training loss 0.006126402129131534\n","2022-03-29 22:50:46.970969 Epoch 231, Training loss 0.00907473252996367\n","2022-03-29 22:51:05.674986 Epoch 232, Training loss 0.005300100274195554\n","2022-03-29 22:51:24.364902 Epoch 233, Training loss 0.003926111887155674\n","2022-03-29 22:51:43.029857 Epoch 234, Training loss 0.0034958624368151793\n","2022-03-29 22:52:01.838035 Epoch 235, Training loss 0.0033045507210251444\n","2022-03-29 22:52:20.785151 Epoch 236, Training loss 0.0031343550865521744\n","2022-03-29 22:52:39.139284 Epoch 237, Training loss 0.003102511710748124\n","2022-03-29 22:52:57.762144 Epoch 238, Training loss 0.0029539117226948784\n","2022-03-29 22:53:16.468185 Epoch 239, Training loss 0.00456731551946254\n","2022-03-29 22:53:35.315890 Epoch 240, Training loss 0.013290787851513849\n","2022-03-29 22:53:54.045200 Epoch 241, Training loss 0.004715430406246768\n","2022-03-29 22:54:12.852996 Epoch 242, Training loss 0.0049189795359626445\n","2022-03-29 22:54:31.612145 Epoch 243, Training loss 0.004024130897090325\n","2022-03-29 22:54:50.362903 Epoch 244, Training loss 0.0031948801725493955\n","2022-03-29 22:55:09.019444 Epoch 245, Training loss 0.005252430965627312\n","2022-03-29 22:55:27.546223 Epoch 246, Training loss 0.002632465279597701\n","2022-03-29 22:55:46.247618 Epoch 247, Training loss 0.0032056328683348415\n","2022-03-29 22:56:04.739561 Epoch 248, Training loss 0.002410350768628013\n","2022-03-29 22:56:23.340636 Epoch 249, Training loss 0.0023461093659699677\n","2022-03-29 22:56:41.892271 Epoch 250, Training loss 0.0022966394416706237\n","2022-03-29 22:57:00.333267 Epoch 251, Training loss 0.0035307713928456773\n","2022-03-29 22:57:18.879897 Epoch 252, Training loss 0.002072941369830084\n","2022-03-29 22:57:37.587825 Epoch 253, Training loss 0.0027730917015298725\n","2022-03-29 22:57:56.424229 Epoch 254, Training loss 0.0020656337547476245\n","2022-03-29 22:58:14.857709 Epoch 255, Training loss 0.0026152029107173493\n","2022-03-29 22:58:33.690173 Epoch 256, Training loss 0.002655365700123991\n","2022-03-29 22:58:52.415280 Epoch 257, Training loss 0.0023755378636863213\n","2022-03-29 22:59:11.081017 Epoch 258, Training loss 0.001691773177113097\n","2022-03-29 22:59:29.467373 Epoch 259, Training loss 0.0019782422592549624\n","2022-03-29 22:59:48.115193 Epoch 260, Training loss 0.002917857648486354\n","2022-03-29 23:00:06.557384 Epoch 261, Training loss 0.002362614484795499\n","2022-03-29 23:00:24.712756 Epoch 262, Training loss 0.0018442653434905176\n","2022-03-29 23:00:43.138694 Epoch 263, Training loss 0.001966807997648996\n","2022-03-29 23:01:01.353220 Epoch 264, Training loss 0.01655231740260485\n","2022-03-29 23:01:20.032392 Epoch 265, Training loss 0.01382512268518\n","2022-03-29 23:01:38.504710 Epoch 266, Training loss 0.005373989660793807\n","2022-03-29 23:01:57.324456 Epoch 267, Training loss 0.0038363446034622066\n","2022-03-29 23:02:16.195289 Epoch 268, Training loss 0.003171502412191135\n","2022-03-29 23:02:34.997399 Epoch 269, Training loss 0.00339191409477415\n","2022-03-29 23:02:53.830128 Epoch 270, Training loss 0.0026556791273503067\n","2022-03-29 23:03:12.704878 Epoch 271, Training loss 0.002395754165696147\n","2022-03-29 23:03:31.836074 Epoch 272, Training loss 0.001771113031477247\n","2022-03-29 23:03:50.728452 Epoch 273, Training loss 0.0017369522252487783\n","2022-03-29 23:04:09.725301 Epoch 274, Training loss 0.0017219440016137493\n","2022-03-29 23:04:28.659220 Epoch 275, Training loss 0.0035042625763277878\n","2022-03-29 23:04:47.412510 Epoch 276, Training loss 0.001807916667581836\n","2022-03-29 23:05:06.646690 Epoch 277, Training loss 0.0019270047254059785\n","2022-03-29 23:05:25.421627 Epoch 278, Training loss 0.001873822627619848\n","2022-03-29 23:05:44.272964 Epoch 279, Training loss 0.001854488557149746\n","2022-03-29 23:06:03.145776 Epoch 280, Training loss 0.03982984886571031\n","2022-03-29 23:06:22.126540 Epoch 281, Training loss 0.2803242992802673\n","2022-03-29 23:06:40.556948 Epoch 282, Training loss 0.21490296432772255\n","2022-03-29 23:06:59.352388 Epoch 283, Training loss 0.1242637319726717\n","2022-03-29 23:07:18.106817 Epoch 284, Training loss 0.07236814679687514\n","2022-03-29 23:07:36.869479 Epoch 285, Training loss 0.06092708236183566\n","2022-03-29 23:07:55.584184 Epoch 286, Training loss 0.043017199626960614\n","2022-03-29 23:08:14.333157 Epoch 287, Training loss 0.03026608818145457\n","2022-03-29 23:08:33.300552 Epoch 288, Training loss 0.011445808414503952\n","2022-03-29 23:08:52.180082 Epoch 289, Training loss 0.010216590251697376\n","2022-03-29 23:09:11.261854 Epoch 290, Training loss 0.00753080564257933\n","2022-03-29 23:09:30.162033 Epoch 291, Training loss 0.00804227939196517\n","2022-03-29 23:09:49.200691 Epoch 292, Training loss 0.004970062027119346\n","2022-03-29 23:10:08.034463 Epoch 293, Training loss 0.006533783658861976\n","2022-03-29 23:10:26.829922 Epoch 294, Training loss 0.004812426708342122\n","2022-03-29 23:10:45.590551 Epoch 295, Training loss 0.003960566641494889\n","2022-03-29 23:11:04.320412 Epoch 296, Training loss 0.006700738083922228\n","2022-03-29 23:11:23.065978 Epoch 297, Training loss 0.0037432693592880082\n","2022-03-29 23:11:42.157507 Epoch 298, Training loss 0.003265389832007774\n","2022-03-29 23:12:01.178635 Epoch 299, Training loss 0.0037052357529737817\n","2022-03-29 23:12:19.728760 Epoch 300, Training loss 0.003602291738288864\n"]}]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n","                                          shuffle=False)\n","val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n","                                        shuffle=False)\n","all_acc_dict[\"ResNet-10\"] = validate(model3, train_loader, val_loader)"],"metadata":{"id":"_NdnEQo8rEqp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648595555283,"user_tz":240,"elapsed":15355,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}},"outputId":"447846a2-e003-47c0-c773-6a8b488cf509"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy train: 1.00\n","Accuracy val: 0.66\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"pMqw21HBEA9G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"FDHMhULLEBA8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##############################################################################\n","################################ Weight Decay ################################\n","##############################################################################"],"metadata":{"id":"zR3gZzS2EBGY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def training_loop_l2reg(n_epochs, optimizer, model, loss_fn,\n","                        train_loader):\n","    for epoch in range(1, n_epochs + 1):\n","        loss_train = 0.0\n","        for imgs, labels in train_loader:\n","            imgs = imgs.to(device=device)\n","            labels = labels.to(device=device)\n","            outputs = model(imgs)\n","            loss = loss_fn(outputs, labels)\n","\n","            l2_lambda = 0.001\n","            l2_norm = sum(p.pow(2.0).sum()\n","                          for p in model.parameters())  # <1>\n","            loss = loss + l2_lambda * l2_norm\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            \n","            loss_train += loss.item()\n","        if epoch == 1 or epoch % 10 == 0:\n","            print('{} Epoch {}, Training loss {}'.format(\n","                datetime.datetime.now(), epoch,\n","                loss_train / len(train_loader)))"],"metadata":{"id":"VQjOmYmWrEwC","executionInfo":{"status":"ok","timestamp":1648608992584,"user_tz":240,"elapsed":326,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n","                                          shuffle=True)\n","model4 = NetResDeep(n_chans1=32, n_blocks=10).to(device=device)\n","optimizer = optim.SGD(model4.parameters(), lr=3e-3)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","training_loop_l2reg(\n","    n_epochs = 300,\n","    optimizer = optimizer,\n","    model = model4,\n","    loss_fn = loss_fn,\n","    train_loader = train_loader,\n",")"],"metadata":{"id":"0wunVSqrrEyz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648584998586,"user_tz":240,"elapsed":5873785,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}},"outputId":"c7c3dd25-5091-4aab-844f-becb797d3c3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-03-29 18:39:03.292240 Epoch 1, Training loss 1.744786892095795\n","2022-03-29 18:41:48.850814 Epoch 10, Training loss 0.978222859180187\n","2022-03-29 18:44:57.207363 Epoch 20, Training loss 0.7765586514530889\n","2022-03-29 18:48:08.945988 Epoch 30, Training loss 0.6537537234823417\n","2022-03-29 18:51:22.857520 Epoch 40, Training loss 0.5563301177662047\n","2022-03-29 18:54:38.924277 Epoch 50, Training loss 0.48478534661443035\n","2022-03-29 18:57:55.105954 Epoch 60, Training loss 0.4269886523904398\n","2022-03-29 19:01:11.653334 Epoch 70, Training loss 0.38006199002647034\n","2022-03-29 19:04:27.492654 Epoch 80, Training loss 0.344297031860065\n","2022-03-29 19:07:44.435463 Epoch 90, Training loss 0.3161183579460434\n","2022-03-29 19:10:59.820952 Epoch 100, Training loss 0.2908612041331618\n","2022-03-29 19:14:16.209084 Epoch 110, Training loss 0.2808811947932024\n","2022-03-29 19:17:33.605346 Epoch 120, Training loss 0.27060127115386834\n","2022-03-29 19:20:49.760146 Epoch 130, Training loss 0.2374589684254983\n","2022-03-29 19:24:05.327807 Epoch 140, Training loss 0.24735392950227497\n","2022-03-29 19:27:23.181545 Epoch 150, Training loss 0.2791493918622851\n","2022-03-29 19:30:39.531565 Epoch 160, Training loss 0.23179929735867874\n","2022-03-29 19:33:57.013576 Epoch 170, Training loss 0.2589240526528005\n","2022-03-29 19:37:13.267765 Epoch 180, Training loss 0.20121002391628598\n","2022-03-29 19:40:30.604404 Epoch 190, Training loss 0.16265217794100648\n","2022-03-29 19:43:48.511356 Epoch 200, Training loss 0.5622631215569004\n","2022-03-29 19:47:07.877548 Epoch 210, Training loss 0.16339751872260247\n","2022-03-29 19:50:25.454666 Epoch 220, Training loss 0.16796534558010223\n","2022-03-29 19:53:43.207965 Epoch 230, Training loss 0.15213870510573277\n","2022-03-29 19:56:59.495762 Epoch 240, Training loss 0.2118682975278181\n","2022-03-29 20:00:17.451124 Epoch 250, Training loss 0.21621875024741263\n","2022-03-29 20:03:34.307314 Epoch 260, Training loss 0.14973966809718506\n","2022-03-29 20:06:50.489333 Epoch 270, Training loss 0.24770090498430344\n","2022-03-29 20:10:08.273974 Epoch 280, Training loss 0.1446647949307166\n","2022-03-29 20:13:22.422959 Epoch 290, Training loss 0.16256949446542793\n","2022-03-29 20:16:38.347319 Epoch 300, Training loss 0.13968054056548707\n"]}]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n","                                          shuffle=False)\n","val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n","                                        shuffle=False)\n","all_acc_dict[\"ResNet-10 Weight Decay\"] = validate(model4, train_loader, val_loader)"],"metadata":{"id":"F635zpYrrE1g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648585013353,"user_tz":240,"elapsed":14779,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}},"outputId":"f03f0e2c-6544-4caa-d997-5c12e190b9b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy train: 0.94\n","Accuracy val: 0.65\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"2RUQsNU3D_se"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"tb60GbcoD_wA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##############################################################################\n","################################## Drop out ##################################\n","##############################################################################"],"metadata":{"id":"o9IN348gD_19"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class NetResDeep(nn.Module):\n","    def __init__(self, n_chans1=32, n_blocks=10):\n","        super().__init__()\n","        self.n_chans1 = n_chans1\n","        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n","        self.conv1_dropout = nn.Dropout2d(p=0.3)\n","        self.resblocks = nn.Sequential(\n","            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n","        self.conv2_dropout = nn.Dropout2d(p=0.3)\n","        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n","        self.fc2 = nn.Linear(32, 10)\n","        \n","    def forward(self, x):\n","        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n","        out = self.conv1_dropout(out)\n","        out = self.resblocks(out)\n","        out = F.max_pool2d(out, 2)\n","        out = self.conv2_dropout(out)\n","        out = out.view(-1, 8 * 8 * self.n_chans1)\n","        out = torch.relu(self.fc1(out))\n","        out = self.fc2(out)\n","        return out"],"metadata":{"id":"v4VteGAenpjq","executionInfo":{"status":"ok","timestamp":1648665612030,"user_tz":240,"elapsed":195,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n","                                          shuffle=True)\n","model5 = NetResDeep(n_chans1=32, n_blocks=10).to(device=device)\n","optimizer = optim.SGD(model5.parameters(), lr=3e-3)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","training_loop(\n","    n_epochs = 300,\n","    optimizer = optimizer,\n","    model = model5,\n","    loss_fn = loss_fn,\n","    train_loader = train_loader,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AHcvc7Qro_uN","executionInfo":{"status":"ok","timestamp":1648669770639,"user_tz":240,"elapsed":4149407,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}},"outputId":"530bda0c-cea8-4cd8-ff70-0922fe768d96"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-03-30 18:40:44.608581 Epoch 1, Training loss 1.9623859780828665\n","2022-03-30 18:40:58.607080 Epoch 2, Training loss 1.6857953951181963\n","2022-03-30 18:41:12.470486 Epoch 3, Training loss 1.5873234758291708\n","2022-03-30 18:41:26.322761 Epoch 4, Training loss 1.5200826545505572\n","2022-03-30 18:41:40.095934 Epoch 5, Training loss 1.4730018250777592\n","2022-03-30 18:41:54.026551 Epoch 6, Training loss 1.426801205900929\n","2022-03-30 18:42:08.018490 Epoch 7, Training loss 1.3862354458140596\n","2022-03-30 18:42:21.776041 Epoch 8, Training loss 1.3533177811014072\n","2022-03-30 18:42:35.579975 Epoch 9, Training loss 1.327164780377122\n","2022-03-30 18:42:49.513873 Epoch 10, Training loss 1.302169436002936\n","2022-03-30 18:43:03.286723 Epoch 11, Training loss 1.270544777669565\n","2022-03-30 18:43:17.041397 Epoch 12, Training loss 1.25134338053596\n","2022-03-30 18:43:31.036857 Epoch 13, Training loss 1.236553414093564\n","2022-03-30 18:43:44.864580 Epoch 14, Training loss 1.2181026420324965\n","2022-03-30 18:43:58.628264 Epoch 15, Training loss 1.2061285963448722\n","2022-03-30 18:44:12.431091 Epoch 16, Training loss 1.1900330288788241\n","2022-03-30 18:44:26.666147 Epoch 17, Training loss 1.1770416790871974\n","2022-03-30 18:44:40.565707 Epoch 18, Training loss 1.1650274335728277\n","2022-03-30 18:44:54.350266 Epoch 19, Training loss 1.1505143948833045\n","2022-03-30 18:45:08.185322 Epoch 20, Training loss 1.1465694673378448\n","2022-03-30 18:45:22.195525 Epoch 21, Training loss 1.134827675569393\n","2022-03-30 18:45:36.304677 Epoch 22, Training loss 1.1176356869890256\n","2022-03-30 18:45:50.227211 Epoch 23, Training loss 1.1083577426955522\n","2022-03-30 18:46:04.058349 Epoch 24, Training loss 1.1066798991559412\n","2022-03-30 18:46:18.057930 Epoch 25, Training loss 1.0954562229726015\n","2022-03-30 18:46:31.932045 Epoch 26, Training loss 1.085177343900856\n","2022-03-30 18:46:45.729567 Epoch 27, Training loss 1.0771092314396977\n","2022-03-30 18:46:59.595396 Epoch 28, Training loss 1.071454824160432\n","2022-03-30 18:47:13.601028 Epoch 29, Training loss 1.0625149634335658\n","2022-03-30 18:47:27.685776 Epoch 30, Training loss 1.059295053982064\n","2022-03-30 18:47:41.561948 Epoch 31, Training loss 1.0508391110945845\n","2022-03-30 18:47:55.270395 Epoch 32, Training loss 1.044721654080369\n","2022-03-30 18:48:09.384548 Epoch 33, Training loss 1.0394402161583571\n","2022-03-30 18:48:23.310377 Epoch 34, Training loss 1.0299205995734086\n","2022-03-30 18:48:37.114910 Epoch 35, Training loss 1.030001772562866\n","2022-03-30 18:48:50.898022 Epoch 36, Training loss 1.02523218038137\n","2022-03-30 18:49:05.063445 Epoch 37, Training loss 1.0181120326909263\n","2022-03-30 18:49:18.674926 Epoch 38, Training loss 1.0131570253225848\n","2022-03-30 18:49:32.456462 Epoch 39, Training loss 1.0049065137305833\n","2022-03-30 18:49:46.352866 Epoch 40, Training loss 1.003314422372052\n","2022-03-30 18:50:00.224143 Epoch 41, Training loss 1.000212792316666\n","2022-03-30 18:50:13.971324 Epoch 42, Training loss 0.9960985166185042\n","2022-03-30 18:50:27.794481 Epoch 43, Training loss 0.9879160141548537\n","2022-03-30 18:50:41.468382 Epoch 44, Training loss 0.986042137798446\n","2022-03-30 18:50:55.574731 Epoch 45, Training loss 0.9814050169399632\n","2022-03-30 18:51:09.510074 Epoch 46, Training loss 0.9836467063183065\n","2022-03-30 18:51:23.336095 Epoch 47, Training loss 0.9759080070059013\n","2022-03-30 18:51:36.944055 Epoch 48, Training loss 0.9664587963877431\n","2022-03-30 18:51:50.827746 Epoch 49, Training loss 0.9685917927328583\n","2022-03-30 18:52:04.569171 Epoch 50, Training loss 0.9595908924289371\n","2022-03-30 18:52:18.334857 Epoch 51, Training loss 0.9564030142238987\n","2022-03-30 18:52:32.130767 Epoch 52, Training loss 0.9592694223231977\n","2022-03-30 18:52:45.979404 Epoch 53, Training loss 0.9502316500677173\n","2022-03-30 18:52:59.984124 Epoch 54, Training loss 0.9486145541033781\n","2022-03-30 18:53:13.760489 Epoch 55, Training loss 0.946197861219611\n","2022-03-30 18:53:27.449193 Epoch 56, Training loss 0.9403480651890835\n","2022-03-30 18:53:41.446504 Epoch 57, Training loss 0.9424735226137254\n","2022-03-30 18:53:55.235344 Epoch 58, Training loss 0.9400794743119604\n","2022-03-30 18:54:09.189407 Epoch 59, Training loss 0.9373025824041927\n","2022-03-30 18:54:22.983364 Epoch 60, Training loss 0.933132541377831\n","2022-03-30 18:54:36.696416 Epoch 61, Training loss 0.9302269331253398\n","2022-03-30 18:54:50.690630 Epoch 62, Training loss 0.926933508776033\n","2022-03-30 18:55:04.583889 Epoch 63, Training loss 0.9215437627356985\n","2022-03-30 18:55:18.277168 Epoch 64, Training loss 0.922923115132105\n","2022-03-30 18:55:32.359861 Epoch 65, Training loss 0.9149617749406859\n","2022-03-30 18:55:46.241942 Epoch 66, Training loss 0.9123195941795779\n","2022-03-30 18:55:59.911011 Epoch 67, Training loss 0.9169447307696428\n","2022-03-30 18:56:13.572429 Epoch 68, Training loss 0.9086557236473883\n","2022-03-30 18:56:27.564923 Epoch 69, Training loss 0.9098477546516281\n","2022-03-30 18:56:41.436690 Epoch 70, Training loss 0.9060597104184768\n","2022-03-30 18:56:55.084043 Epoch 71, Training loss 0.9080748433042365\n","2022-03-30 18:57:08.942856 Epoch 72, Training loss 0.8996839112866565\n","2022-03-30 18:57:22.701081 Epoch 73, Training loss 0.9016757909294284\n","2022-03-30 18:57:36.482957 Epoch 74, Training loss 0.8928596328591447\n","2022-03-30 18:57:50.213684 Epoch 75, Training loss 0.8934462615443618\n","2022-03-30 18:58:03.983585 Epoch 76, Training loss 0.894611932623112\n","2022-03-30 18:58:17.806178 Epoch 77, Training loss 0.8889910435432669\n","2022-03-30 18:58:31.573260 Epoch 78, Training loss 0.8848064124126873\n","2022-03-30 18:58:45.227571 Epoch 79, Training loss 0.8893155720075379\n","2022-03-30 18:58:59.207169 Epoch 80, Training loss 0.8851134759919418\n","2022-03-30 18:59:13.014719 Epoch 81, Training loss 0.8856315758374645\n","2022-03-30 18:59:26.816800 Epoch 82, Training loss 0.8794172188205183\n","2022-03-30 18:59:40.703997 Epoch 83, Training loss 0.8782609561672601\n","2022-03-30 18:59:54.453491 Epoch 84, Training loss 0.876900522452791\n","2022-03-30 19:00:08.152154 Epoch 85, Training loss 0.8765109100610095\n","2022-03-30 19:00:21.865381 Epoch 86, Training loss 0.8770093622110079\n","2022-03-30 19:00:35.521802 Epoch 87, Training loss 0.8705418604566618\n","2022-03-30 19:00:49.207193 Epoch 88, Training loss 0.8703724104730065\n","2022-03-30 19:01:03.191084 Epoch 89, Training loss 0.8713234991521177\n","2022-03-30 19:01:17.085322 Epoch 90, Training loss 0.8698456743946466\n","2022-03-30 19:01:30.818068 Epoch 91, Training loss 0.8675639486831167\n","2022-03-30 19:01:44.700753 Epoch 92, Training loss 0.8695463137248592\n","2022-03-30 19:01:58.652190 Epoch 93, Training loss 0.8609842994176519\n","2022-03-30 19:02:12.407773 Epoch 94, Training loss 0.8598455611778342\n","2022-03-30 19:02:26.257537 Epoch 95, Training loss 0.859402292410431\n","2022-03-30 19:02:39.865785 Epoch 96, Training loss 0.8568326815619798\n","2022-03-30 19:02:53.677236 Epoch 97, Training loss 0.8590079946133792\n","2022-03-30 19:03:07.439725 Epoch 98, Training loss 0.8551595189687237\n","2022-03-30 19:03:21.316573 Epoch 99, Training loss 0.8498488142514777\n","2022-03-30 19:03:35.255635 Epoch 100, Training loss 0.8539307589649849\n","2022-03-30 19:03:49.103031 Epoch 101, Training loss 0.8495987032532997\n","2022-03-30 19:04:03.503577 Epoch 102, Training loss 0.8499197421400139\n","2022-03-30 19:04:17.272223 Epoch 103, Training loss 0.8487237631664861\n","2022-03-30 19:04:31.236038 Epoch 104, Training loss 0.8510776002465001\n","2022-03-30 19:04:45.022239 Epoch 105, Training loss 0.838924760556282\n","2022-03-30 19:04:58.796584 Epoch 106, Training loss 0.8389856287890383\n","2022-03-30 19:05:12.521179 Epoch 107, Training loss 0.8383331692127316\n","2022-03-30 19:05:26.120635 Epoch 108, Training loss 0.8383681688391035\n","2022-03-30 19:05:39.883801 Epoch 109, Training loss 0.8413021588112082\n","2022-03-30 19:05:53.593412 Epoch 110, Training loss 0.8368739861890179\n","2022-03-30 19:06:07.247746 Epoch 111, Training loss 0.8334381994231582\n","2022-03-30 19:06:21.136500 Epoch 112, Training loss 0.8360970327463906\n","2022-03-30 19:06:34.845295 Epoch 113, Training loss 0.8323808318513739\n","2022-03-30 19:06:48.382146 Epoch 114, Training loss 0.8358985544241908\n","2022-03-30 19:07:02.130564 Epoch 115, Training loss 0.8263495253479999\n","2022-03-30 19:07:15.915525 Epoch 116, Training loss 0.828895812754131\n","2022-03-30 19:07:29.782052 Epoch 117, Training loss 0.8231351392348404\n","2022-03-30 19:07:43.472659 Epoch 118, Training loss 0.8323072485073143\n","2022-03-30 19:07:57.142167 Epoch 119, Training loss 0.8283900367024609\n","2022-03-30 19:08:10.729387 Epoch 120, Training loss 0.8231373883193106\n","2022-03-30 19:08:24.495561 Epoch 121, Training loss 0.8239190203835592\n","2022-03-30 19:08:38.202225 Epoch 122, Training loss 0.8220421852129499\n","2022-03-30 19:08:52.132161 Epoch 123, Training loss 0.8238911463324067\n","2022-03-30 19:09:05.775578 Epoch 124, Training loss 0.8202216530699864\n","2022-03-30 19:09:19.555012 Epoch 125, Training loss 0.8148862343767415\n","2022-03-30 19:09:33.182208 Epoch 126, Training loss 0.8185372766860001\n","2022-03-30 19:09:47.051306 Epoch 127, Training loss 0.8125674279830645\n","2022-03-30 19:10:00.907203 Epoch 128, Training loss 0.8167906130671196\n","2022-03-30 19:10:14.698603 Epoch 129, Training loss 0.815749353764917\n","2022-03-30 19:10:28.382072 Epoch 130, Training loss 0.8077203199610381\n","2022-03-30 19:10:42.054795 Epoch 131, Training loss 0.8128840286103661\n","2022-03-30 19:10:55.857953 Epoch 132, Training loss 0.8108464400176807\n","2022-03-30 19:11:09.656089 Epoch 133, Training loss 0.8094521600876927\n","2022-03-30 19:11:23.431393 Epoch 134, Training loss 0.8039440143367519\n","2022-03-30 19:11:37.433582 Epoch 135, Training loss 0.8047204128921489\n","2022-03-30 19:11:51.183109 Epoch 136, Training loss 0.8090953679798204\n","2022-03-30 19:12:04.745621 Epoch 137, Training loss 0.8049069711619325\n","2022-03-30 19:12:18.487794 Epoch 138, Training loss 0.801600089120438\n","2022-03-30 19:12:32.218563 Epoch 139, Training loss 0.800055717339601\n","2022-03-30 19:12:46.005397 Epoch 140, Training loss 0.7953885857711362\n","2022-03-30 19:12:59.691771 Epoch 141, Training loss 0.7965185518764779\n","2022-03-30 19:13:13.526736 Epoch 142, Training loss 0.7953377263167934\n","2022-03-30 19:13:27.150154 Epoch 143, Training loss 0.7932823966531193\n","2022-03-30 19:13:40.879499 Epoch 144, Training loss 0.798603530925558\n","2022-03-30 19:13:54.542260 Epoch 145, Training loss 0.7922500124977677\n","2022-03-30 19:14:08.382669 Epoch 146, Training loss 0.7915603804313923\n","2022-03-30 19:14:22.242789 Epoch 147, Training loss 0.7916454132408133\n","2022-03-30 19:14:36.065541 Epoch 148, Training loss 0.7868683169717374\n","2022-03-30 19:14:49.680703 Epoch 149, Training loss 0.7863187368416116\n","2022-03-30 19:15:03.659458 Epoch 150, Training loss 0.7861432162925716\n","2022-03-30 19:15:17.571309 Epoch 151, Training loss 0.7846796139884178\n","2022-03-30 19:15:31.335563 Epoch 152, Training loss 0.7859891553592804\n","2022-03-30 19:15:45.032730 Epoch 153, Training loss 0.7809729895856984\n","2022-03-30 19:15:58.987168 Epoch 154, Training loss 0.7837422458107209\n","2022-03-30 19:16:12.557441 Epoch 155, Training loss 0.7775561937971798\n","2022-03-30 19:16:26.267655 Epoch 156, Training loss 0.7802652090864108\n","2022-03-30 19:16:40.200350 Epoch 157, Training loss 0.7812988509226333\n","2022-03-30 19:16:53.979498 Epoch 158, Training loss 0.7747422977329215\n","2022-03-30 19:17:07.792632 Epoch 159, Training loss 0.7753183428207626\n","2022-03-30 19:17:21.501846 Epoch 160, Training loss 0.7715564570234864\n","2022-03-30 19:17:35.246605 Epoch 161, Training loss 0.773568794626714\n","2022-03-30 19:17:49.271918 Epoch 162, Training loss 0.7739702347294449\n","2022-03-30 19:18:03.061616 Epoch 163, Training loss 0.7659481090048085\n","2022-03-30 19:18:16.753258 Epoch 164, Training loss 0.7742473382474212\n","2022-03-30 19:18:30.574189 Epoch 165, Training loss 0.7674385377436953\n","2022-03-30 19:18:44.469887 Epoch 166, Training loss 0.76588499740414\n","2022-03-30 19:18:58.036396 Epoch 167, Training loss 0.7717695028504448\n","2022-03-30 19:19:11.713831 Epoch 168, Training loss 0.7682935874480421\n","2022-03-30 19:19:25.686239 Epoch 169, Training loss 0.7682703547846631\n","2022-03-30 19:19:39.602576 Epoch 170, Training loss 0.7647353097072342\n","2022-03-30 19:19:53.402152 Epoch 171, Training loss 0.766269999544334\n","2022-03-30 19:20:06.936299 Epoch 172, Training loss 0.7602354144043935\n","2022-03-30 19:20:20.871210 Epoch 173, Training loss 0.7611694641963905\n","2022-03-30 19:20:34.731673 Epoch 174, Training loss 0.761318392918238\n","2022-03-30 19:20:48.395199 Epoch 175, Training loss 0.7614790206141484\n","2022-03-30 19:21:02.339243 Epoch 176, Training loss 0.7570991301170701\n","2022-03-30 19:21:16.303414 Epoch 177, Training loss 0.7574937215546513\n","2022-03-30 19:21:29.955133 Epoch 178, Training loss 0.7587320400625849\n","2022-03-30 19:21:43.613694 Epoch 179, Training loss 0.7573954031595489\n","2022-03-30 19:21:57.400164 Epoch 180, Training loss 0.752097394612744\n","2022-03-30 19:22:11.216453 Epoch 181, Training loss 0.7534994832466325\n","2022-03-30 19:22:24.908460 Epoch 182, Training loss 0.753889248194292\n","2022-03-30 19:22:38.693266 Epoch 183, Training loss 0.7523143757182313\n","2022-03-30 19:22:52.267932 Epoch 184, Training loss 0.7494786111137751\n","2022-03-30 19:23:06.196725 Epoch 185, Training loss 0.7474688949716061\n","2022-03-30 19:23:19.987648 Epoch 186, Training loss 0.7512148166708934\n","2022-03-30 19:23:33.656337 Epoch 187, Training loss 0.7484241153501794\n","2022-03-30 19:23:47.511448 Epoch 188, Training loss 0.7484953123742663\n","2022-03-30 19:24:01.459018 Epoch 189, Training loss 0.7502388469017375\n","2022-03-30 19:24:15.090927 Epoch 190, Training loss 0.7499136267720586\n","2022-03-30 19:24:28.944911 Epoch 191, Training loss 0.7468699304877645\n","2022-03-30 19:24:42.932967 Epoch 192, Training loss 0.745595883530424\n","2022-03-30 19:24:56.774013 Epoch 193, Training loss 0.7454271795956985\n","2022-03-30 19:25:10.526291 Epoch 194, Training loss 0.7426473376772288\n","2022-03-30 19:25:24.414628 Epoch 195, Training loss 0.744510214956825\n","2022-03-30 19:25:38.177335 Epoch 196, Training loss 0.7420558525473261\n","2022-03-30 19:25:52.198846 Epoch 197, Training loss 0.7391094113783458\n","2022-03-30 19:26:05.812798 Epoch 198, Training loss 0.7442517376997891\n","2022-03-30 19:26:19.650457 Epoch 199, Training loss 0.7365636542782454\n","2022-03-30 19:26:33.605270 Epoch 200, Training loss 0.741852320216196\n","2022-03-30 19:26:47.473106 Epoch 201, Training loss 0.7381445314268322\n","2022-03-30 19:27:01.124742 Epoch 202, Training loss 0.7431072349019368\n","2022-03-30 19:27:14.850154 Epoch 203, Training loss 0.7407628186904561\n","2022-03-30 19:27:28.761872 Epoch 204, Training loss 0.740371519151856\n","2022-03-30 19:27:42.774384 Epoch 205, Training loss 0.7351514826650205\n","2022-03-30 19:27:56.588231 Epoch 206, Training loss 0.7362315504980819\n","2022-03-30 19:28:10.364309 Epoch 207, Training loss 0.7344727976166684\n","2022-03-30 19:28:24.089798 Epoch 208, Training loss 0.7310651412324223\n","2022-03-30 19:28:37.705538 Epoch 209, Training loss 0.7320951634584485\n","2022-03-30 19:28:51.450043 Epoch 210, Training loss 0.7327698312909402\n","2022-03-30 19:29:05.561842 Epoch 211, Training loss 0.7299290222432607\n","2022-03-30 19:29:19.394626 Epoch 212, Training loss 0.7282960832957417\n","2022-03-30 19:29:33.126560 Epoch 213, Training loss 0.727776767576442\n","2022-03-30 19:29:46.728407 Epoch 214, Training loss 0.7233213004096389\n","2022-03-30 19:30:00.550814 Epoch 215, Training loss 0.7288336655901521\n","2022-03-30 19:30:14.274119 Epoch 216, Training loss 0.7289343878741155\n","2022-03-30 19:30:27.966129 Epoch 217, Training loss 0.7332284658613717\n","2022-03-30 19:30:41.706120 Epoch 218, Training loss 0.7246614797112277\n","2022-03-30 19:30:55.303929 Epoch 219, Training loss 0.7207040419359036\n","2022-03-30 19:31:09.089254 Epoch 220, Training loss 0.7208643858618748\n","2022-03-30 19:31:22.813715 Epoch 221, Training loss 0.7215424106096673\n","2022-03-30 19:31:36.578187 Epoch 222, Training loss 0.7170966593429561\n","2022-03-30 19:31:50.340579 Epoch 223, Training loss 0.7170409287333184\n","2022-03-30 19:32:04.047487 Epoch 224, Training loss 0.7207564437938163\n","2022-03-30 19:32:17.803819 Epoch 225, Training loss 0.7256157002256959\n","2022-03-30 19:32:31.594404 Epoch 226, Training loss 0.7200385612218886\n","2022-03-30 19:32:45.671458 Epoch 227, Training loss 0.7235961545001516\n","2022-03-30 19:32:59.484687 Epoch 228, Training loss 0.7163830005832951\n","2022-03-30 19:33:13.282774 Epoch 229, Training loss 0.7254130593727312\n","2022-03-30 19:33:27.013283 Epoch 230, Training loss 0.7187647511587119\n","2022-03-30 19:33:40.560112 Epoch 231, Training loss 0.7129295668791017\n","2022-03-30 19:33:54.304509 Epoch 232, Training loss 0.7153481987431226\n","2022-03-30 19:34:08.038113 Epoch 233, Training loss 0.71638125169765\n","2022-03-30 19:34:22.043130 Epoch 234, Training loss 0.7135078554872967\n","2022-03-30 19:34:35.844966 Epoch 235, Training loss 0.7112277463421492\n","2022-03-30 19:34:49.680493 Epoch 236, Training loss 0.7091799762166674\n","2022-03-30 19:35:03.172906 Epoch 237, Training loss 0.7121510488526596\n","2022-03-30 19:35:16.983593 Epoch 238, Training loss 0.7144011105494121\n","2022-03-30 19:35:30.779022 Epoch 239, Training loss 0.7182154270922742\n","2022-03-30 19:35:44.454704 Epoch 240, Training loss 0.7100343506049622\n","2022-03-30 19:35:58.442153 Epoch 241, Training loss 0.7104666317286699\n","2022-03-30 19:36:12.101275 Epoch 242, Training loss 0.7075006701909673\n","2022-03-30 19:36:25.709123 Epoch 243, Training loss 0.7064174962851703\n","2022-03-30 19:36:39.579118 Epoch 244, Training loss 0.7051302564647192\n","2022-03-30 19:36:53.396674 Epoch 245, Training loss 0.707822568162018\n","2022-03-30 19:37:07.229698 Epoch 246, Training loss 0.7005882565780064\n","2022-03-30 19:37:21.191395 Epoch 247, Training loss 0.7036979306689308\n","2022-03-30 19:37:34.865646 Epoch 248, Training loss 0.7007597974880272\n","2022-03-30 19:37:48.644896 Epoch 249, Training loss 0.7048407846399586\n","2022-03-30 19:38:02.278468 Epoch 250, Training loss 0.7037999899795903\n","2022-03-30 19:38:16.291373 Epoch 251, Training loss 0.698607956700008\n","2022-03-30 19:38:30.196525 Epoch 252, Training loss 0.6994386271137716\n","2022-03-30 19:38:44.163184 Epoch 253, Training loss 0.7056535861223859\n","2022-03-30 19:38:57.811341 Epoch 254, Training loss 0.7046491251043652\n","2022-03-30 19:39:11.496054 Epoch 255, Training loss 0.7026118404801239\n","2022-03-30 19:39:25.244854 Epoch 256, Training loss 0.7004176210183317\n","2022-03-30 19:39:39.063074 Epoch 257, Training loss 0.7002424527617062\n","2022-03-30 19:39:52.968884 Epoch 258, Training loss 0.6990707596702039\n","2022-03-30 19:40:06.892662 Epoch 259, Training loss 0.6915682100731394\n","2022-03-30 19:40:20.474831 Epoch 260, Training loss 0.6982857172598924\n","2022-03-30 19:40:34.356152 Epoch 261, Training loss 0.696944527880615\n","2022-03-30 19:40:48.222364 Epoch 262, Training loss 0.6921042182942485\n","2022-03-30 19:41:02.040742 Epoch 263, Training loss 0.7027660903266019\n","2022-03-30 19:41:15.940042 Epoch 264, Training loss 0.694101643569939\n","2022-03-30 19:41:29.637044 Epoch 265, Training loss 0.6932101500461169\n","2022-03-30 19:41:43.175814 Epoch 266, Training loss 0.6939046276957178\n","2022-03-30 19:41:57.141848 Epoch 267, Training loss 0.6974111484825763\n","2022-03-30 19:42:11.044683 Epoch 268, Training loss 0.6970340804675655\n","2022-03-30 19:42:24.792813 Epoch 269, Training loss 0.6913561484469172\n","2022-03-30 19:42:38.667911 Epoch 270, Training loss 0.6935114086131611\n","2022-03-30 19:42:52.520309 Epoch 271, Training loss 0.6889425894945783\n","2022-03-30 19:43:06.115670 Epoch 272, Training loss 0.6903629491243826\n","2022-03-30 19:43:19.747560 Epoch 273, Training loss 0.6872112590181249\n","2022-03-30 19:43:33.485357 Epoch 274, Training loss 0.6891922001414896\n","2022-03-30 19:43:47.172620 Epoch 275, Training loss 0.6931129429498901\n","2022-03-30 19:44:01.183338 Epoch 276, Training loss 0.6883956152764733\n","2022-03-30 19:44:14.815858 Epoch 277, Training loss 0.6861524656224434\n","2022-03-30 19:44:28.360479 Epoch 278, Training loss 0.6908739071596613\n","2022-03-30 19:44:42.124652 Epoch 279, Training loss 0.6900742445379267\n","2022-03-30 19:44:55.878267 Epoch 280, Training loss 0.6866595319774754\n","2022-03-30 19:45:09.677180 Epoch 281, Training loss 0.6864078217912513\n","2022-03-30 19:45:23.332334 Epoch 282, Training loss 0.6860273914492648\n","2022-03-30 19:45:36.985391 Epoch 283, Training loss 0.6848485360060201\n","2022-03-30 19:45:50.690599 Epoch 284, Training loss 0.6858092526264508\n","2022-03-30 19:46:04.561881 Epoch 285, Training loss 0.6840844331571209\n","2022-03-30 19:46:18.410936 Epoch 286, Training loss 0.684457584072257\n","2022-03-30 19:46:32.147823 Epoch 287, Training loss 0.6830913997672098\n","2022-03-30 19:46:45.896269 Epoch 288, Training loss 0.6878511469306239\n","2022-03-30 19:46:59.770469 Epoch 289, Training loss 0.687381905858474\n","2022-03-30 19:47:13.348809 Epoch 290, Training loss 0.6800109761983842\n","2022-03-30 19:47:26.930575 Epoch 291, Training loss 0.6904729438559783\n","2022-03-30 19:47:40.612709 Epoch 292, Training loss 0.679360740515582\n","2022-03-30 19:47:54.492735 Epoch 293, Training loss 0.6809909090666515\n","2022-03-30 19:48:08.327746 Epoch 294, Training loss 0.6845144115369338\n","2022-03-30 19:48:21.869163 Epoch 295, Training loss 0.6851005216541193\n","2022-03-30 19:48:35.487043 Epoch 296, Training loss 0.6824570453685262\n","2022-03-30 19:48:49.434343 Epoch 297, Training loss 0.6795345061956464\n","2022-03-30 19:49:03.253238 Epoch 298, Training loss 0.67827289747765\n","2022-03-30 19:49:16.878933 Epoch 299, Training loss 0.6829543745197604\n","2022-03-30 19:49:30.660118 Epoch 300, Training loss 0.676333986127468\n"]}]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n","                                          shuffle=False)\n","val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n","                                        shuffle=False)\n","all_acc_dict[\"ResNet-10 Weight Decay\"] = validate(model5, train_loader, val_loader)"],"metadata":{"id":"5TUQYcIOvdRt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648669782469,"user_tz":240,"elapsed":11834,"user":{"displayName":"Tyler Chambers","userId":"08287771444136751052"}},"outputId":"1da50984-f647-4782-9da5-f43ce5381ded"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy train: 0.76\n","Accuracy val: 0.67\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Homework3.ipynb","provenance":[],"authorship_tag":"ABX9TyMimIiL42RwnYVYKNq+fgXN"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b46c110ab99647cbb0d54b172e1f5f52":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c578e227d654c70a841f46b08881c5a","IPY_MODEL_92e1ace7c79f4c619f311eef71f383c4","IPY_MODEL_80c77ad358cd43869ef908b0daf37292"],"layout":"IPY_MODEL_8b829b6cb33943f4b1e6cd5d0514a958"}},"7c578e227d654c70a841f46b08881c5a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aff27db74d6442578637dbe7e124e47d","placeholder":"","style":"IPY_MODEL_c0d18a3c21d34bb197729f04ecfa96ea","value":""}},"92e1ace7c79f4c619f311eef71f383c4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a809c516a02449d79956d84755e02590","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_887bf1b12d0644c1a726aa66b133517b","value":170498071}},"80c77ad358cd43869ef908b0daf37292":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e088ab8ecb424a4dba5544b04940f0fa","placeholder":"","style":"IPY_MODEL_a6947ee658274be39957504c92d6037b","value":" 170499072/? [00:03&lt;00:00, 55339512.26it/s]"}},"8b829b6cb33943f4b1e6cd5d0514a958":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aff27db74d6442578637dbe7e124e47d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0d18a3c21d34bb197729f04ecfa96ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a809c516a02449d79956d84755e02590":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"887bf1b12d0644c1a726aa66b133517b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e088ab8ecb424a4dba5544b04940f0fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6947ee658274be39957504c92d6037b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}